{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# fix the random seed\n",
    "torch.manual_seed(55272025)\n",
    "\n",
    "dtype = torch.float\n",
    "w = 8"
   ],
   "metadata": {
    "id": "BSRr9krmAuB5",
    "ExecuteTime": {
     "end_time": "2025-03-21T07:09:32.367095Z",
     "start_time": "2025-03-21T07:09:30.955758Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rosenbrock"
   ],
   "metadata": {
    "id": "miojWLbdfRUt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Rosenbrock with equality constraint\n",
    "def f(x1, x2):\n",
    "    return w * torch.abs(x1 ** 2 - x2) + (1 - x1) ** 2\n",
    "\n",
    "def penalty(x1, x2):\n",
    "    return torch.abs(math.sqrt(2) * x1 - 1) + torch.abs(2 * x2 - 1)\n",
    "\n",
    "def phi1(x1, x2, mu):\n",
    "    return f(x1, x2) + mu * penalty(x1, x2)"
   ],
   "metadata": {
    "id": "Tma-roqGAXBE",
    "ExecuteTime": {
     "end_time": "2025-03-21T07:09:32.371010Z",
     "start_time": "2025-03-21T07:09:32.368300Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ypyyvqq4AJIQ",
    "ExecuteTime": {
     "end_time": "2025-03-21T07:09:32.376943Z",
     "start_time": "2025-03-21T07:09:32.371975Z"
    }
   },
   "source": [
    "def backtracking_line_search(t_rho, c, x_eps, mu_rho, mu_eps):\n",
    "    x1 = torch.randn(1, 1, dtype=dtype, requires_grad=True)\n",
    "    x2 = torch.randn(1, 1, dtype=dtype, requires_grad=True)\n",
    "    mu = torch.tensor([1.], dtype=dtype)\n",
    "\n",
    "    for iteration in range(1000):\n",
    "        print(\"Iter\", iteration)\n",
    "\n",
    "        # Find an approximate minimizer xk of phi1(x; µk), starting at xk^s\n",
    "        for epoch in range(100):\n",
    "            phi1_x_mu = phi1(x1, x2, mu)\n",
    "            phi1_x_mu.backward()\n",
    "\n",
    "            # if epoch % 100 == 99:\n",
    "            print(\"Epoch\", epoch)\n",
    "            print(\"Objective + penalty:\", phi1_x_mu.item())\n",
    "            print(\"Vars:\", x1, x2, mu)\n",
    "            print(\"Grads:\", torch.norm(x1.grad), torch.norm(x2.grad))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if torch.norm(x1.grad) ** 2 + torch.norm(x2.grad) ** 2 <= x_eps:\n",
    "                    break\n",
    "\n",
    "                t = 1\n",
    "                while t >= 1e-8 and phi1(x1 - t * x1.grad, x2 - t * x2.grad, mu) - phi1_x_mu >= -c * t * (torch.norm(x1.grad) ** 2 + torch.norm(x2.grad) ** 2):\n",
    "                    t *= t_rho\n",
    "                print(\"t:\", t)\n",
    "                if t < 1e-8:\n",
    "                    break\n",
    "\n",
    "                x1 -= t * x1.grad\n",
    "                x1.grad = None\n",
    "                x2 -= t * x2.grad\n",
    "                x2.grad = None\n",
    "\n",
    "        h = penalty(x1, x2)\n",
    "        print(\"Penalty\", h)\n",
    "        if h < 1e-5:  # if h(xk ) ≤ τ\n",
    "            break\n",
    "\n",
    "        # Choose new penalty parameter µk+1 > µk ;\n",
    "        if mu * h > mu_eps:\n",
    "            mu *= mu_rho\n",
    "\n",
    "        # Choose new starting point (stay as optimal x1, x2)\n",
    "\n",
    "        print()\n",
    "\n",
    "    return x1, x2"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "backtracking_line_search(t_rho=0.8, c=0, x_eps=1e-5, mu_rho=1.1, mu_eps=1e-5)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmPMnY1ICSsR",
    "outputId": "14608986-66e7-4505-ea11-67b48a7ce51c",
    "ExecuteTime": {
     "end_time": "2025-03-21T07:09:32.973974Z",
     "start_time": "2025-03-21T07:09:32.378945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "Epoch 0\n",
      "Objective + penalty: 9.709888458251953\n",
      "Vars: tensor([[-1.0277]], requires_grad=True) tensor([[0.7176]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(21.9136) tensor(6.)\n",
      "t: 0.10737418240000006\n",
      "Epoch 1\n",
      "Objective + penalty: 5.858539581298828\n",
      "Vars: tensor([[1.3252]], requires_grad=True) tensor([[1.3618]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(23.2680) tensor(6.)\n",
      "t: 0.011529215046068483\n",
      "Epoch 2\n",
      "Objective + penalty: 4.8707075119018555\n",
      "Vars: tensor([[1.0570]], requires_grad=True) tensor([[1.4310]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(15.3831) tensor(10.)\n",
      "t: 0.011529215046068483\n",
      "Epoch 3\n",
      "Objective + penalty: 4.094383239746094\n",
      "Vars: tensor([[1.2343]], requires_grad=True) tensor([[1.3157]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(21.6317) tensor(6.)\n",
      "t: 0.00737869762948383\n",
      "Epoch 4\n",
      "Objective + penalty: 3.8853907585144043\n",
      "Vars: tensor([[1.0747]], requires_grad=True) tensor([[1.3600]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(15.6315) tensor(10.)\n",
      "t: 0.00737869762948383\n",
      "Epoch 5\n",
      "Objective + penalty: 3.3313918113708496\n",
      "Vars: tensor([[1.1900]], requires_grad=True) tensor([[1.2862]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(20.8348) tensor(6.)\n",
      "t: 0.004722366482869652\n",
      "Epoch 6\n",
      "Objective + penalty: 3.163892984390259\n",
      "Vars: tensor([[1.0916]], requires_grad=True) tensor([[1.3145]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(15.8688) tensor(10.)\n",
      "t: 0.004722366482869652\n",
      "Epoch 7\n",
      "Objective + penalty: 2.9610753059387207\n",
      "Vars: tensor([[1.1666]], requires_grad=True) tensor([[1.2673]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(20.4127) tensor(6.)\n",
      "t: 0.0037778931862957215\n",
      "Epoch 8\n",
      "Objective + penalty: 2.9528698921203613\n",
      "Vars: tensor([[1.0895]], requires_grad=True) tensor([[1.2900]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(15.8383) tensor(10.)\n",
      "t: 0.0037778931862957215\n",
      "Epoch 9\n",
      "Objective + penalty: 2.7016706466674805\n",
      "Vars: tensor([[1.1493]], requires_grad=True) tensor([[1.2522]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(20.1016) tensor(6.)\n",
      "t: 0.002417851639229262\n",
      "Epoch 10\n",
      "Objective + penalty: 2.5413622856140137\n",
      "Vars: tensor([[1.1007]], requires_grad=True) tensor([[1.2667]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(15.9955) tensor(10.)\n",
      "t: 0.0019342813113834097\n",
      "Epoch 11\n",
      "Objective + penalty: 2.3784401416778564\n",
      "Vars: tensor([[1.1316]], requires_grad=True) tensor([[1.2473]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.7837) tensor(6.)\n",
      "t: 0.0012379400392853823\n",
      "Epoch 12\n",
      "Objective + penalty: 2.318767786026001\n",
      "Vars: tensor([[1.1071]], requires_grad=True) tensor([[1.2548]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.0858) tensor(10.)\n",
      "t: 0.0012379400392853823\n",
      "Epoch 13\n",
      "Objective + penalty: 2.317777395248413\n",
      "Vars: tensor([[1.1271]], requires_grad=True) tensor([[1.2424]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.7013) tensor(6.)\n",
      "t: 0.0009903520314283058\n",
      "Epoch 14\n",
      "Objective + penalty: 2.2479259967803955\n",
      "Vars: tensor([[1.1075]], requires_grad=True) tensor([[1.2483]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.0914) tensor(10.)\n",
      "t: 0.0007922816251426447\n",
      "Epoch 15\n",
      "Objective + penalty: 2.1968472003936768\n",
      "Vars: tensor([[1.1203]], requires_grad=True) tensor([[1.2404]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.5795) tensor(6.)\n",
      "t: 0.0005070602400912927\n",
      "Epoch 16\n",
      "Objective + penalty: 2.153671979904175\n",
      "Vars: tensor([[1.1104]], requires_grad=True) tensor([[1.2435]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1309) tensor(10.)\n",
      "t: 0.00040564819207303417\n",
      "Epoch 17\n",
      "Objective + penalty: 2.1367690563201904\n",
      "Vars: tensor([[1.1169]], requires_grad=True) tensor([[1.2394]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.5186) tensor(6.)\n",
      "t: 0.00032451855365842736\n",
      "Epoch 18\n",
      "Objective + penalty: 2.129202127456665\n",
      "Vars: tensor([[1.1106]], requires_grad=True) tensor([[1.2413]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1339) tensor(10.)\n",
      "t: 0.00032451855365842736\n",
      "Epoch 19\n",
      "Objective + penalty: 2.123128652572632\n",
      "Vars: tensor([[1.1158]], requires_grad=True) tensor([[1.2381]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4988) tensor(6.)\n",
      "t: 0.0002596148429267419\n",
      "Epoch 20\n",
      "Objective + penalty: 2.1095266342163086\n",
      "Vars: tensor([[1.1108]], requires_grad=True) tensor([[1.2397]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1363) tensor(10.)\n",
      "t: 0.00020769187434139353\n",
      "Epoch 21\n",
      "Objective + penalty: 2.092910051345825\n",
      "Vars: tensor([[1.1141]], requires_grad=True) tensor([[1.2376]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4681) tensor(6.)\n",
      "t: 0.00013292279957849188\n",
      "Epoch 22\n",
      "Objective + penalty: 2.0844080448150635\n",
      "Vars: tensor([[1.1115]], requires_grad=True) tensor([[1.2384]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1470) tensor(10.)\n",
      "t: 0.0001063382396627935\n",
      "Epoch 23\n",
      "Objective + penalty: 2.0775606632232666\n",
      "Vars: tensor([[1.1132]], requires_grad=True) tensor([[1.2373]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4524) tensor(6.)\n",
      "t: 6.805647338418785e-05\n",
      "Epoch 24\n",
      "Objective + penalty: 2.0715060234069824\n",
      "Vars: tensor([[1.1119]], requires_grad=True) tensor([[1.2377]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1525) tensor(10.)\n",
      "t: 5.444517870735028e-05\n",
      "Epoch 25\n",
      "Objective + penalty: 2.0697338581085205\n",
      "Vars: tensor([[1.1128]], requires_grad=True) tensor([[1.2372]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4444) tensor(6.)\n",
      "t: 4.3556142965880224e-05\n",
      "Epoch 26\n",
      "Objective + penalty: 2.0681493282318115\n",
      "Vars: tensor([[1.1119]], requires_grad=True) tensor([[1.2374]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1529) tensor(10.)\n",
      "t: 4.3556142965880224e-05\n",
      "Epoch 27\n",
      "Objective + penalty: 2.0679945945739746\n",
      "Vars: tensor([[1.1126]], requires_grad=True) tensor([[1.2370]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4418) tensor(6.)\n",
      "t: 3.484491437270418e-05\n",
      "Epoch 28\n",
      "Objective + penalty: 2.065463066101074\n",
      "Vars: tensor([[1.1120]], requires_grad=True) tensor([[1.2372]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1533) tensor(10.)\n",
      "t: 2.7875931498163346e-05\n",
      "Epoch 29\n",
      "Objective + penalty: 2.063995122909546\n",
      "Vars: tensor([[1.1124]], requires_grad=True) tensor([[1.2369]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4377) tensor(6.)\n",
      "t: 2.2300745198530677e-05\n",
      "Epoch 30\n",
      "Objective + penalty: 2.0637412071228027\n",
      "Vars: tensor([[1.1120]], requires_grad=True) tensor([[1.2371]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1535) tensor(10.)\n",
      "t: 2.2300745198530677e-05\n",
      "Epoch 31\n",
      "Objective + penalty: 2.063108205795288\n",
      "Vars: tensor([[1.1123]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4364) tensor(6.)\n",
      "t: 1.784059615882454e-05\n",
      "Epoch 32\n",
      "Objective + penalty: 2.062364101409912\n",
      "Vars: tensor([[1.1120]], requires_grad=True) tensor([[1.2370]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1537) tensor(10.)\n",
      "t: 1.4272476927059634e-05\n",
      "Epoch 33\n",
      "Objective + penalty: 2.061063051223755\n",
      "Vars: tensor([[1.1122]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4343) tensor(6.)\n",
      "t: 9.134385233318167e-06\n",
      "Epoch 34\n",
      "Objective + penalty: 2.0606284141540527\n",
      "Vars: tensor([[1.1120]], requires_grad=True) tensor([[1.2369]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1545) tensor(10.)\n",
      "t: 7.307508186654534e-06\n",
      "Epoch 35\n",
      "Objective + penalty: 2.060016632080078\n",
      "Vars: tensor([[1.1122]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4332) tensor(6.)\n",
      "t: 4.676805239458903e-06\n",
      "Epoch 36\n",
      "Objective + penalty: 2.0597386360168457\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1549) tensor(10.)\n",
      "t: 3.7414441915671226e-06\n",
      "Epoch 37\n",
      "Objective + penalty: 2.059481620788574\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4327) tensor(6.)\n",
      "t: 2.394524282602959e-06\n",
      "Epoch 38\n",
      "Objective + penalty: 2.0592832565307617\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 1.9156194260823675e-06\n",
      "Epoch 39\n",
      "Objective + penalty: 2.0592093467712402\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4324) tensor(6.)\n",
      "t: 1.5324955408658941e-06\n",
      "Epoch 40\n",
      "Objective + penalty: 2.059164524078369\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 1.5324955408658941e-06\n",
      "Epoch 41\n",
      "Objective + penalty: 2.0591495037078857\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4323) tensor(6.)\n",
      "t: 1.2259964326927154e-06\n",
      "Epoch 42\n",
      "Objective + penalty: 2.0590689182281494\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 9.807971461541723e-07\n",
      "Epoch 43\n",
      "Objective + penalty: 2.0590083599090576\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4322) tensor(6.)\n",
      "t: 7.846377169233379e-07\n",
      "Epoch 44\n",
      "Objective + penalty: 2.0590081214904785\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 7.846377169233379e-07\n",
      "Epoch 45\n",
      "Objective + penalty: 2.058976888656616\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4321) tensor(6.)\n",
      "t: 6.277101735386704e-07\n",
      "Epoch 46\n",
      "Objective + penalty: 2.0589599609375\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 6.277101735386704e-07\n",
      "Epoch 47\n",
      "Objective + penalty: 2.0589523315429688\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4321) tensor(6.)\n",
      "t: 5.021681388309363e-07\n",
      "Epoch 48\n",
      "Objective + penalty: 2.058920383453369\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 4.017345110647491e-07\n",
      "Epoch 49\n",
      "Objective + penalty: 2.058894157409668\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4320) tensor(6.)\n",
      "t: 2.5711008708143947e-07\n",
      "Epoch 50\n",
      "Objective + penalty: 2.058872938156128\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 2.056880696651516e-07\n",
      "Epoch 51\n",
      "Objective + penalty: 2.0588645935058594\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4320) tensor(6.)\n",
      "t: 1.645504557321213e-07\n",
      "Epoch 52\n",
      "Objective + penalty: 2.058859348297119\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 1.645504557321213e-07\n",
      "Epoch 53\n",
      "Objective + penalty: 2.0588574409484863\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4320) tensor(6.)\n",
      "t: 1.3164036458569703e-07\n",
      "Epoch 54\n",
      "Objective + penalty: 2.058849334716797\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 1.0531229166855763e-07\n",
      "Epoch 55\n",
      "Objective + penalty: 2.058842420578003\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4320) tensor(6.)\n",
      "t: 6.739986666787689e-08\n",
      "Epoch 56\n",
      "Objective + penalty: 2.0588362216949463\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 5.3919893334301516e-08\n",
      "Epoch 57\n",
      "Objective + penalty: 2.0588345527648926\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4320) tensor(6.)\n",
      "t: 4.313591466744121e-08\n",
      "Epoch 58\n",
      "Objective + penalty: 2.0588326454162598\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 3.450873173395297e-08\n",
      "Epoch 59\n",
      "Objective + penalty: 2.058830738067627\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4320) tensor(6.)\n",
      "t: 2.7606985387162378e-08\n",
      "Epoch 60\n",
      "Objective + penalty: 2.0588302612304688\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 2.7606985387162378e-08\n",
      "Epoch 61\n",
      "Objective + penalty: 2.058828353881836\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4320) tensor(6.)\n",
      "t: 1.7668470647783922e-08\n",
      "Epoch 62\n",
      "Objective + penalty: 2.0588274002075195\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 1.7668470647783922e-08\n",
      "Epoch 63\n",
      "Objective + penalty: 2.058826446533203\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4320) tensor(6.)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 1\n",
      "Epoch 0\n",
      "Objective + penalty: 2.2634522914886475\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.1000])\n",
      "Grads: tensor(39.0053) tensor(11.8000)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 2\n",
      "Epoch 0\n",
      "Objective + penalty: 2.4885408878326416\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.2100])\n",
      "Grads: tensor(58.7343) tensor(17.3800)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 3\n",
      "Epoch 0\n",
      "Objective + penalty: 2.736138343811035\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.3310])\n",
      "Grads: tensor(78.6343) tensor(22.7180)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 4\n",
      "Epoch 0\n",
      "Objective + penalty: 3.008495569229126\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.4641])\n",
      "Grads: tensor(98.7226) tensor(27.7898)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 5\n",
      "Epoch 0\n",
      "Objective + penalty: 3.3080883026123047\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.6105])\n",
      "Grads: tensor(119.0179) tensor(32.5688)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 6\n",
      "Epoch 0\n",
      "Objective + penalty: 3.6376402378082275\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.7716])\n",
      "Grads: tensor(139.5410) tensor(37.0257)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 7\n",
      "Epoch 0\n",
      "Objective + penalty: 4.000147819519043\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.9487])\n",
      "Grads: tensor(160.3147) tensor(41.1282)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 8\n",
      "Epoch 0\n",
      "Objective + penalty: 4.3989057540893555\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([2.1436])\n",
      "Grads: tensor(181.3639) tensor(44.8410)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 9\n",
      "Epoch 0\n",
      "Objective + penalty: 4.8375396728515625\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([2.3579])\n",
      "Grads: tensor(202.7163) tensor(48.1251)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 10\n",
      "Epoch 0\n",
      "Objective + penalty: 5.320036888122559\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([2.5937])\n",
      "Grads: tensor(224.4021) tensor(50.9377)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 11\n",
      "Epoch 0\n",
      "Objective + penalty: 5.8507843017578125\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([2.8531])\n",
      "Grads: tensor(246.4548) tensor(53.2314)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 12\n",
      "Epoch 0\n",
      "Objective + penalty: 6.434605598449707\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([3.1384])\n",
      "Grads: tensor(268.9109) tensor(54.9546)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 13\n",
      "Epoch 0\n",
      "Objective + penalty: 7.076809883117676\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([3.4523])\n",
      "Grads: tensor(291.8109) tensor(56.0500)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 14\n",
      "Epoch 0\n",
      "Objective + penalty: 7.783234119415283\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([3.7975])\n",
      "Grads: tensor(315.1992) tensor(56.4550)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 15\n",
      "Epoch 0\n",
      "Objective + penalty: 8.560300827026367\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([4.1772])\n",
      "Grads: tensor(339.1244) tensor(56.1005)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 16\n",
      "Epoch 0\n",
      "Objective + penalty: 9.415074348449707\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([4.5950])\n",
      "Grads: tensor(363.6404) tensor(54.9106)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 17\n",
      "Epoch 0\n",
      "Objective + penalty: 10.355324745178223\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([5.0545])\n",
      "Grads: tensor(388.8063) tensor(52.8016)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 18\n",
      "Epoch 0\n",
      "Objective + penalty: 11.38960075378418\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([5.5599])\n",
      "Grads: tensor(414.6869) tensor(49.6818)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 19\n",
      "Epoch 0\n",
      "Objective + penalty: 12.527304649353027\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([6.1159])\n",
      "Grads: tensor(441.3539) tensor(45.4500)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 20\n",
      "Epoch 0\n",
      "Objective + penalty: 13.778779029846191\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([6.7275])\n",
      "Grads: tensor(468.8857) tensor(39.9950)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 21\n",
      "Epoch 0\n",
      "Objective + penalty: 15.155401229858398\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([7.4003])\n",
      "Grads: tensor(497.3690) tensor(33.1945)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 22\n",
      "Epoch 0\n",
      "Objective + penalty: 16.6696834564209\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([8.1403])\n",
      "Grads: tensor(526.8988) tensor(24.9139)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 23\n",
      "Epoch 0\n",
      "Objective + penalty: 18.33539581298828\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([8.9543])\n",
      "Grads: tensor(557.5798) tensor(15.0053)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 24\n",
      "Epoch 0\n",
      "Objective + penalty: 20.167678833007812\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([9.8497])\n",
      "Grads: tensor(589.5272) tensor(3.3058)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 25\n",
      "Epoch 0\n",
      "Objective + penalty: 22.183189392089844\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(622.8676) tensor(10.3636)\n",
      "t: 0.0006338253001141158\n",
      "Epoch 1\n",
      "Objective + penalty: 21.784282684326172\n",
      "Vars: tensor([[0.7173]], requires_grad=True) tensor([[1.2302]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.2803) tensor(29.6694)\n",
      "t: 0.043980465111040035\n",
      "Epoch 2\n",
      "Objective + penalty: 17.914127349853516\n",
      "Vars: tensor([[0.5730]], requires_grad=True) tensor([[-0.0747]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(7.0079) tensor(29.6694)\n",
      "t: 0.03518437208883203\n",
      "Epoch 3\n",
      "Objective + penalty: 14.303622245788574\n",
      "Vars: tensor([[0.8196]], requires_grad=True) tensor([[0.9692]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(1.8481) tensor(29.6694)\n",
      "t: 0.028147497671065627\n",
      "Epoch 4\n",
      "Objective + penalty: 12.55046558380127\n",
      "Vars: tensor([[0.7676]], requires_grad=True) tensor([[0.1341]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.1392) tensor(29.6694)\n",
      "t: 0.014411518807585602\n",
      "Epoch 5\n",
      "Objective + penalty: 10.15103816986084\n",
      "Vars: tensor([[0.3765]], requires_grad=True) tensor([[0.5617]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(22.5932) tensor(29.6694)\n",
      "t: 0.011529215046068483\n",
      "Epoch 6\n",
      "Objective + penalty: 8.771390914916992\n",
      "Vars: tensor([[0.6370]], requires_grad=True) tensor([[0.2196]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(5.8574) tensor(29.6694)\n",
      "t: 0.018014398509482003\n",
      "Epoch 7\n",
      "Objective + penalty: 7.736728668212891\n",
      "Vars: tensor([[0.7425]], requires_grad=True) tensor([[0.7541]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(2.9280) tensor(29.6694)\n",
      "t: 0.014411518807585602\n",
      "Epoch 8\n",
      "Objective + penalty: 5.265050888061523\n",
      "Vars: tensor([[0.7003]], requires_grad=True) tensor([[0.3265]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.7177) tensor(29.6694)\n",
      "t: 0.011529215046068483\n",
      "Epoch 9\n",
      "Objective + penalty: 5.234173774719238\n",
      "Vars: tensor([[0.7547]], requires_grad=True) tensor([[0.6686]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(2.7573) tensor(29.6694)\n",
      "t: 0.009223372036854787\n",
      "Epoch 10\n",
      "Objective + penalty: 3.7842869758605957\n",
      "Vars: tensor([[0.7292]], requires_grad=True) tensor([[0.3949]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.4488) tensor(29.6694)\n",
      "t: 0.0037778931862957215\n",
      "Epoch 11\n",
      "Objective + penalty: 2.369042158126831\n",
      "Vars: tensor([[0.6293]], requires_grad=True) tensor([[0.5070]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.1330) tensor(29.6694)\n",
      "t: 0.002417851639229262\n",
      "Epoch 12\n",
      "Objective + penalty: 2.0752930641174316\n",
      "Vars: tensor([[0.6925]], requires_grad=True) tensor([[0.4353]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.8576) tensor(29.6694)\n",
      "t: 0.0037778931862957215\n",
      "Epoch 13\n",
      "Objective + penalty: 1.5036189556121826\n",
      "Vars: tensor([[0.7109]], requires_grad=True) tensor([[0.5474]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.3707) tensor(29.6694)\n",
      "t: 0.0030223145490365774\n",
      "Epoch 14\n",
      "Objective + penalty: 1.3711178302764893\n",
      "Vars: tensor([[0.7007]], requires_grad=True) tensor([[0.4577]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.7107) tensor(29.6694)\n",
      "t: 0.002417851639229262\n",
      "Epoch 15\n",
      "Objective + penalty: 0.9755687117576599\n",
      "Vars: tensor([[0.7121]], requires_grad=True) tensor([[0.5294]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.3539) tensor(29.6694)\n",
      "t: 0.0019342813113834097\n",
      "Epoch 16\n",
      "Objective + penalty: 0.9225664138793945\n",
      "Vars: tensor([[0.7056]], requires_grad=True) tensor([[0.4720]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.6224) tensor(29.6694)\n",
      "t: 0.0015474250491067279\n",
      "Epoch 17\n",
      "Objective + penalty: 0.6372565031051636\n",
      "Vars: tensor([[0.7127]], requires_grad=True) tensor([[0.5179]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.3445) tensor(29.6694)\n",
      "t: 0.0009903520314283058\n",
      "Epoch 18\n",
      "Objective + penalty: 0.4850235879421234\n",
      "Vars: tensor([[0.7094]], requires_grad=True) tensor([[0.4886]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0919) tensor(29.6694)\n",
      "t: 0.00040564819207303417\n",
      "Epoch 19\n",
      "Objective + penalty: 0.3286277949810028\n",
      "Vars: tensor([[0.6988]], requires_grad=True) tensor([[0.5006]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.1061) tensor(29.6694)\n",
      "t: 0.0002596148429267419\n",
      "Epoch 20\n",
      "Objective + penalty: 0.30228346586227417\n",
      "Vars: tensor([[0.7059]], requires_grad=True) tensor([[0.4929]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.6171) tensor(29.6694)\n",
      "t: 0.00040564819207303417\n",
      "Epoch 21\n",
      "Objective + penalty: 0.23425498604774475\n",
      "Vars: tensor([[0.7077]], requires_grad=True) tensor([[0.5049]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.4143) tensor(29.6694)\n",
      "t: 0.00032451855365842736\n",
      "Epoch 22\n",
      "Objective + penalty: 0.22734040021896362\n",
      "Vars: tensor([[0.7066]], requires_grad=True) tensor([[0.4953]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.6034) tensor(29.6694)\n",
      "t: 0.0002596148429267419\n",
      "Epoch 23\n",
      "Objective + penalty: 0.17742066085338593\n",
      "Vars: tensor([[0.7078]], requires_grad=True) tensor([[0.5030]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.4131) tensor(29.6694)\n",
      "t: 0.00016615349947311482\n",
      "Epoch 24\n",
      "Objective + penalty: 0.1466250717639923\n",
      "Vars: tensor([[0.7073]], requires_grad=True) tensor([[0.4981]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0531) tensor(29.6694)\n",
      "t: 6.805647338418785e-05\n",
      "Epoch 25\n",
      "Objective + penalty: 0.13293392956256866\n",
      "Vars: tensor([[0.7055]], requires_grad=True) tensor([[0.5001]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.1993) tensor(29.6694)\n",
      "t: 5.444517870735028e-05\n",
      "Epoch 26\n",
      "Objective + penalty: 0.1315421164035797\n",
      "Vars: tensor([[0.7070]], requires_grad=True) tensor([[0.4985]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.5973) tensor(29.6694)\n",
      "t: 8.507059173023481e-05\n",
      "Epoch 27\n",
      "Objective + penalty: 0.11642804741859436\n",
      "Vars: tensor([[0.7074]], requires_grad=True) tensor([[0.5010]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.4197) tensor(29.6694)\n",
      "t: 6.805647338418785e-05\n",
      "Epoch 28\n",
      "Objective + penalty: 0.1161799281835556\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.4990]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0507) tensor(29.6694)\n",
      "t: 3.484491437270418e-05\n",
      "Epoch 29\n",
      "Objective + penalty: 0.11075765639543533\n",
      "Vars: tensor([[0.7062]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2095) tensor(29.6694)\n",
      "t: 2.7875931498163346e-05\n",
      "Epoch 30\n",
      "Objective + penalty: 0.11038868874311447\n",
      "Vars: tensor([[0.7070]], requires_grad=True) tensor([[0.4992]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.5972) tensor(29.6694)\n",
      "t: 5.444517870735028e-05\n",
      "Epoch 31\n",
      "Objective + penalty: 0.11012877523899078\n",
      "Vars: tensor([[0.7072]], requires_grad=True) tensor([[0.5008]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.4215) tensor(29.6694)\n",
      "t: 5.444517870735028e-05\n",
      "Epoch 32\n",
      "Objective + penalty: 0.11009452491998672\n",
      "Vars: tensor([[0.7070]], requires_grad=True) tensor([[0.4992]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.5960) tensor(29.6694)\n",
      "t: 4.3556142965880224e-05\n",
      "Epoch 33\n",
      "Objective + penalty: 0.10059225559234619\n",
      "Vars: tensor([[0.7072]], requires_grad=True) tensor([[0.5005]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.4213) tensor(29.6694)\n",
      "t: 2.7875931498163346e-05\n",
      "Epoch 34\n",
      "Objective + penalty: 0.09675335884094238\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.4997]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0511) tensor(29.6694)\n",
      "t: 1.1417981541647708e-05\n",
      "Epoch 35\n",
      "Objective + penalty: 0.09309656172990799\n",
      "Vars: tensor([[0.7068]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2184) tensor(13.6694)\n",
      "t: 1.4272476927059634e-05\n",
      "Epoch 36\n",
      "Objective + penalty: 0.09187860041856766\n",
      "Vars: tensor([[0.7072]], requires_grad=True) tensor([[0.5002]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.4214) tensor(29.6694)\n",
      "t: 9.134385233318167e-06\n",
      "Epoch 37\n",
      "Objective + penalty: 0.09051366150379181\n",
      "Vars: tensor([[0.7072]], requires_grad=True) tensor([[0.4999]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0521) tensor(29.6694)\n",
      "t: 5.846006549323628e-06\n",
      "Epoch 38\n",
      "Objective + penalty: 0.09024471044540405\n",
      "Vars: tensor([[0.7070]], requires_grad=True) tensor([[0.5001]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2212) tensor(29.6694)\n",
      "t: 4.676805239458903e-06\n",
      "Epoch 39\n",
      "Objective + penalty: 0.08883397281169891\n",
      "Vars: tensor([[0.7072]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0517) tensor(29.6694)\n",
      "t: 3.7414441915671226e-06\n",
      "Epoch 40\n",
      "Objective + penalty: 0.08860741555690765\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5001]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2216) tensor(29.6694)\n",
      "t: 2.9931553532536984e-06\n",
      "Epoch 41\n",
      "Objective + penalty: 0.0877593532204628\n",
      "Vars: tensor([[0.7072]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0514) tensor(29.6694)\n",
      "t: 2.394524282602959e-06\n",
      "Epoch 42\n",
      "Objective + penalty: 0.08756157755851746\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2219) tensor(29.6694)\n",
      "t: 1.9156194260823675e-06\n",
      "Epoch 43\n",
      "Objective + penalty: 0.08707143366336823\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0512) tensor(29.6694)\n",
      "t: 1.5324955408658941e-06\n",
      "Epoch 44\n",
      "Objective + penalty: 0.08689107745885849\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2221) tensor(29.6694)\n",
      "t: 1.2259964326927154e-06\n",
      "Epoch 45\n",
      "Objective + penalty: 0.08664853125810623\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0511) tensor(13.6694)\n",
      "t: 1.9156194260823675e-06\n",
      "Epoch 46\n",
      "Objective + penalty: 0.08662974089384079\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.5950) tensor(29.6694)\n",
      "t: 1.2259964326927154e-06\n",
      "Epoch 47\n",
      "Objective + penalty: 0.08640985190868378\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2219) tensor(29.6694)\n",
      "t: 6.277101735386704e-07\n",
      "Epoch 48\n",
      "Objective + penalty: 0.08617406338453293\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0506) tensor(29.6694)\n",
      "t: 4.017345110647491e-07\n",
      "Epoch 49\n",
      "Objective + penalty: 0.08602988719940186\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2220) tensor(29.6694)\n",
      "t: 2.5711008708143947e-07\n",
      "Epoch 50\n",
      "Objective + penalty: 0.08595667779445648\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0506) tensor(29.6694)\n",
      "t: 2.056880696651516e-07\n",
      "Epoch 51\n",
      "Objective + penalty: 0.08594030141830444\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2220) tensor(29.6694)\n",
      "t: 1.645504557321213e-07\n",
      "Epoch 52\n",
      "Objective + penalty: 0.08589760959148407\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0505) tensor(29.6694)\n",
      "t: 1.3164036458569703e-07\n",
      "Epoch 53\n",
      "Objective + penalty: 0.0858844742178917\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2221) tensor(29.6694)\n",
      "t: 1.0531229166855763e-07\n",
      "Epoch 54\n",
      "Objective + penalty: 0.08585836738348007\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0505) tensor(29.6694)\n",
      "t: 8.42498333348461e-08\n",
      "Epoch 55\n",
      "Objective + penalty: 0.08584731817245483\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2221) tensor(29.6694)\n",
      "t: 6.739986666787689e-08\n",
      "Epoch 56\n",
      "Objective + penalty: 0.08583544939756393\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0505) tensor(29.6694)\n",
      "t: 5.3919893334301516e-08\n",
      "Epoch 57\n",
      "Objective + penalty: 0.08582370728254318\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2221) tensor(29.6694)\n",
      "t: 4.313591466744121e-08\n",
      "Epoch 58\n",
      "Objective + penalty: 0.08582001179456711\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0505) tensor(29.6694)\n",
      "t: 3.450873173395297e-08\n",
      "Epoch 59\n",
      "Objective + penalty: 0.08580860495567322\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2221) tensor(29.6694)\n",
      "t: 2.2085588309729903e-08\n",
      "Epoch 60\n",
      "Objective + penalty: 0.08580470830202103\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.5947) tensor(29.6694)\n",
      "t: 3.450873173395297e-08\n",
      "Epoch 61\n",
      "Objective + penalty: 0.08579830825328827\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(11.8995) tensor(29.6694)\n",
      "t: 1.7668470647783922e-08\n",
      "Epoch 62\n",
      "Objective + penalty: 0.085797019302845\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0505) tensor(29.6694)\n",
      "t: 1.1307821214581712e-08\n",
      "Epoch 63\n",
      "Objective + penalty: 0.08579440414905548\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2221) tensor(29.6694)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[5.3644e-07]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7071]], requires_grad=True),\n",
       " tensor([[0.5000]], requires_grad=True))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sphere Manifold"
   ],
   "metadata": {
    "id": "zJrVf8TqfKzK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cpu')\n",
    "torch.manual_seed(0)\n",
    "n = 300\n",
    "# All the user-provided data (vector/matrix/tensor) must be in torch tensor format.\n",
    "# As PyTorch tensor is single precision by default, one must explicitly set `dtype=torch.double`.\n",
    "# Also, please make sure the device of provided torch tensor is the same as opts.torch_device.\n",
    "dtype = torch.double\n",
    "A = torch.randn((n,n)).to(device=device, dtype=dtype)\n",
    "A = .5*(A+A.T)"
   ],
   "metadata": {
    "id": "MVteRjGtD116",
    "ExecuteTime": {
     "end_time": "2025-03-21T07:29:17.499489Z",
     "start_time": "2025-03-21T07:29:17.483466Z"
    }
   },
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "source": [
    "def f(x):\n",
    "    return -x.T @ A @ x\n",
    "\n",
    "def penalty(x):\n",
    "    return torch.abs(x.T @ x - 1)\n",
    "\n",
    "def phi1(x, mu):\n",
    "    return f(x) + mu * penalty(x)"
   ],
   "metadata": {
    "id": "fBlOQFtrfNJu",
    "ExecuteTime": {
     "end_time": "2025-03-21T07:29:17.923152Z",
     "start_time": "2025-03-21T07:29:17.921012Z"
    }
   },
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "source": [
    "def backtracking_line_search(t_rho, c, x_eps, mu_rho, mu_eps):\n",
    "    x = torch.randn(n, 1, dtype=dtype, requires_grad=True)\n",
    "    with torch.no_grad():\n",
    "        x /= torch.norm(x)\n",
    "    mu = torch.tensor([100.], dtype=dtype)\n",
    "\n",
    "    for iteration in range(1000):\n",
    "        print(\"Iter\", iteration)\n",
    "\n",
    "        # Find an approximate minimizer xk of phi1(x; µk), starting at xk^s\n",
    "        for epoch in range(1000):\n",
    "            phi1_x_mu = phi1(x, mu)\n",
    "            phi1_x_mu.backward()\n",
    "\n",
    "            # if epoch % 100 == 99:\n",
    "            # print(\"Epoch\", epoch)\n",
    "            # print(\"Objective + penalty:\", phi1_x_mu.item())\n",
    "            # print(\"Vars:\", x, mu)\n",
    "            # print(\"Grads:\", torch.norm(x.grad))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if torch.norm(x.grad) ** 2 <= x_eps:\n",
    "                    break\n",
    "\n",
    "                t = 1\n",
    "                while t >= 1e-8 and phi1(x - t * x.grad, mu) - phi1_x_mu >= -c * t * torch.norm(x.grad) ** 2:\n",
    "                    t *= t_rho\n",
    "                print(\"t:\", t, torch.norm(x.grad))\n",
    "                # if t < 1e-8:\n",
    "                #     break\n",
    "\n",
    "                x -= t * x.grad\n",
    "                x.grad = None\n",
    "\n",
    "        h = penalty(x)\n",
    "        print(\"Objective\", phi1_x_mu)\n",
    "        print(\"Penalty\", h)\n",
    "        if h < 1e-5:  # if h(xk ) ≤ τ\n",
    "            break\n",
    "\n",
    "        # Choose new penalty parameter µk+1 > µk ;\n",
    "        if mu * h > mu_eps:\n",
    "            mu *= mu_rho\n",
    "\n",
    "        # Choose new starting point (stay as optimal x1, x2)\n",
    "\n",
    "        print()\n",
    "\n",
    "    return x"
   ],
   "metadata": {
    "id": "ak3FJwkFfdT5",
    "ExecuteTime": {
     "end_time": "2025-03-21T07:29:18.085430Z",
     "start_time": "2025-03-21T07:29:18.081038Z"
    }
   },
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "source": [
    "x = backtracking_line_search(t_rho=0.8, c=0, x_eps=1e-5, mu_rho=1.1, mu_eps=1e-5)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9lfugKPhgKhd",
    "outputId": "c64992c0-1f4d-4736-d07e-d4ff9791e7a1",
    "ExecuteTime": {
     "end_time": "2025-03-21T07:29:20.455903Z",
     "start_time": "2025-03-21T07:29:18.248809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "t: 9.046256971665371e-09 tensor(200.7635, dtype=torch.float64)\n",
      "t: 1.7668470647783922e-08 tensor(202.1281, dtype=torch.float64)\n",
      "t: 1.7668470647783922e-08 tensor(200.7638, dtype=torch.float64)\n",
      "t: 1.7668470647783922e-08 tensor(202.1281, dtype=torch.float64)\n",
      "t: 1.7668470647783922e-08 tensor(200.7638, dtype=torch.float64)\n",
      "t: 1.7668470647783922e-08 tensor(202.1282, dtype=torch.float64)\n",
      "t: 1.7668470647783922e-08 tensor(200.7637, dtype=torch.float64)\n",
      "t: 1.4134776518227139e-08 tensor(202.1282, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7635, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1284, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7635, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1284, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7635, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1284, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7635, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1284, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7634, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1285, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7633, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1285, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7633, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1285, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7633, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1285, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7633, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1286, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7633, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1286, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7633, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1286, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7632, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1286, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7632, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1286, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7632, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1287, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7632, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1287, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7632, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1287, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7631, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1287, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7631, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1288, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7631, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1288, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7631, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1288, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7631, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1288, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7631, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1289, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7630, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1289, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7630, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1289, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7630, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1289, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7630, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1289, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7630, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1290, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7629, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1290, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7629, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1290, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7629, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1290, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7629, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1291, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7629, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1291, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7629, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1291, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7628, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1291, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7628, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1292, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7628, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1292, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7628, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1292, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7628, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1291, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7627, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1292, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7627, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1292, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7626, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1292, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7626, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1292, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7626, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1293, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7625, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1293, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7625, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1293, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7625, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1293, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7625, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1294, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7625, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1294, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7625, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1294, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7624, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1294, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7624, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1295, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7624, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1295, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7624, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1295, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7624, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1295, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7623, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1295, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7623, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1296, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7623, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1296, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7623, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1296, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7623, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1296, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7623, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1297, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7622, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1297, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7622, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1297, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7622, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1297, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7622, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1298, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7622, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1298, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7621, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1298, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7621, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1298, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7621, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1299, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7621, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1299, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7621, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1299, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7621, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1299, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7620, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1299, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7620, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1300, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7620, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1299, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7620, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1299, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7619, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1300, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7618, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1300, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7618, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1300, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7618, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1300, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7618, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1301, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7618, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1301, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7618, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1301, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7617, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1301, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7617, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1301, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7617, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1302, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7617, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1302, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7617, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1302, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7616, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1302, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7616, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1303, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7616, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1303, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7616, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1303, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7616, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1303, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7615, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1304, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7615, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1304, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7615, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1304, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7615, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1304, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7615, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1304, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7615, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1305, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7614, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1305, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7614, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1305, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7614, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1305, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7614, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1306, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7614, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1306, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7613, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1306, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7613, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1306, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7613, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1307, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7613, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1307, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7613, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1307, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7613, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1306, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7612, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1307, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7612, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1307, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7611, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1307, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7611, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1307, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7611, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1308, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7610, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1308, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7610, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1308, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7610, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1308, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7610, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1309, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7610, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1309, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7610, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1309, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7609, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1309, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7609, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1310, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7609, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1310, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7609, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1310, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7609, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1310, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7608, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1310, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7608, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1311, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7608, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1311, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7608, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1311, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7608, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1311, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7607, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1312, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7607, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1312, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7607, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1312, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7607, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1312, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7607, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1313, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7607, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1313, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7606, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1313, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7606, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1313, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7606, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1314, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7606, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1314, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7606, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1314, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7605, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1314, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7605, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1314, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7605, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1315, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7605, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1314, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7605, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1314, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7604, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1315, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7603, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1315, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7603, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1315, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7603, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1315, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7603, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1316, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7603, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1316, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7602, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1316, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7602, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1316, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7602, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1316, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7602, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1317, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7602, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1317, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7601, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1317, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7601, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1317, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7601, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1318, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7601, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1318, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7601, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1318, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7601, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1318, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7600, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1319, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7600, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1319, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7600, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1319, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7600, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1319, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7600, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1320, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7599, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1320, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7599, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1320, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7599, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1320, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7599, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1320, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7599, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1321, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7599, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1321, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7598, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1321, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7598, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1321, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7598, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1322, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7598, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1322, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7598, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1322, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7597, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1322, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7597, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1322, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7597, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1322, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7596, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1322, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7596, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1322, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7596, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1323, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7595, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1323, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7595, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1323, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7595, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1323, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7595, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1324, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7595, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1324, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7594, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1324, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7594, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1324, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7594, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1325, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7594, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1325, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7594, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1325, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7594, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1325, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7593, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1325, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7593, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1326, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7593, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1326, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7593, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1326, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7593, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1326, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7592, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1327, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7592, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1327, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7592, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1327, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7592, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1327, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7592, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1328, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7592, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1328, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7591, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1328, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7591, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1328, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7591, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1328, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7591, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1329, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7591, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1329, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7590, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1329, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7590, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1329, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7590, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1330, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7590, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1329, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7590, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1329, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7589, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1330, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7588, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1330, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7588, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1330, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7588, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1330, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7588, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1331, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7588, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1331, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7587, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1331, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7587, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1331, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7587, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1331, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7587, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1332, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7587, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1332, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7586, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1332, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7586, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1332, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7586, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1333, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7586, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1333, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7586, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1333, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7586, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1333, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7585, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1334, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7585, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1334, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7585, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1334, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7585, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1334, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7585, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1334, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7584, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1335, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7584, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1335, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7584, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1335, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7584, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1335, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7584, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1336, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7584, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1336, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7583, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1336, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7583, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1336, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7583, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1337, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7583, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1337, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7583, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1337, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7582, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1337, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7582, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1337, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7582, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1337, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7582, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1337, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7581, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1337, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7580, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1338, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7580, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1338, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7580, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1338, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7580, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1338, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7580, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1339, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7580, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1339, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7579, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1339, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7579, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1339, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7579, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1340, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7579, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1340, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7579, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1340, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7578, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1340, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7578, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1340, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7578, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1341, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7578, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1341, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7578, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1341, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7578, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1341, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7577, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1342, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7577, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1342, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7577, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1342, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7577, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1342, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7577, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1343, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7576, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1343, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7576, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1343, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7576, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1343, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7576, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1344, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7576, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1344, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7576, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1344, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7575, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1344, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7575, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1344, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7575, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1345, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7575, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1344, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7575, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1344, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7574, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1345, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7573, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1345, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7573, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1345, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7573, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1345, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7573, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1346, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7572, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1346, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7572, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1346, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7572, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1346, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7572, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1346, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7572, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1347, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7572, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1347, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7571, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1347, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7571, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1347, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7571, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1348, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7571, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1348, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7571, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1348, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7570, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1348, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7570, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1349, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7570, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1349, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7570, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1349, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7570, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1349, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7570, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1350, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7569, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1350, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7569, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1350, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7569, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1350, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7569, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1350, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7569, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1351, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7568, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1351, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7568, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1351, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7568, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1351, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7568, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1352, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7568, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1352, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7567, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1352, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7567, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1352, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7567, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1352, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7567, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1352, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7566, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1352, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7566, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1352, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7565, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1353, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7565, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1353, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7565, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1353, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7565, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1353, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7565, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1354, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7564, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1354, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7564, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1354, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7564, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1354, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7564, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1355, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7564, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1355, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7564, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1355, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7563, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1355, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7563, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1355, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7563, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1356, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7563, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1356, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7563, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1356, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7562, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1356, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7562, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1357, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7562, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1357, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7562, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1357, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7562, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1357, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7562, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1358, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7561, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1358, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7561, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1358, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7561, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1358, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7561, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1359, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7561, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1359, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7560, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1359, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7560, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1359, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7560, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1359, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7560, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1360, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7560, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1359, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7559, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1359, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7559, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1360, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7558, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1360, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7558, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1360, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7558, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1360, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7558, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1361, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7557, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1361, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7557, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1361, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7557, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1361, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7557, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1361, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7557, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1362, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7556, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1362, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7556, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1362, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7556, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1362, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7556, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1363, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7556, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1363, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7556, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1363, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7555, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1363, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7555, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1364, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7555, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1364, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7555, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1364, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7555, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1364, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7554, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1365, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7554, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1365, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7554, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1365, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7554, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1365, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7554, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1365, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7554, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1366, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7553, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1366, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7553, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1366, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7553, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1366, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7553, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1367, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7553, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1367, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7552, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1367, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7552, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1367, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7552, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1367, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7552, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1367, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7551, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1367, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7551, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1367, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7550, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1368, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7550, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1368, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7550, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1368, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7550, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1368, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7550, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1369, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7549, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1369, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7549, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1369, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7549, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1369, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7549, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1370, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7549, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1370, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7549, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1370, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7548, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1370, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7548, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1370, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7548, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1371, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7548, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1371, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7548, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1371, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7547, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1371, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7547, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1372, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7547, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1372, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7547, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1372, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7547, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1372, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7547, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1373, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7546, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1373, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7546, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1373, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7546, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1373, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7546, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1374, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7546, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1374, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7545, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1374, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7545, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1374, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7545, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1374, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7545, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1375, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7545, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1374, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7544, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1374, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7544, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1375, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7543, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1375, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7543, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1375, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7543, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1375, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7543, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1376, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7542, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1376, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7542, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1376, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7542, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1376, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7542, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1376, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7542, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1377, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7541, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1377, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7541, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1377, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7541, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1377, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7541, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1378, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7541, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1378, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7541, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1378, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7540, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1378, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7540, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1379, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7540, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1379, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7540, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1379, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7540, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1379, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7539, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1380, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7539, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1380, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7539, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1380, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7539, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1380, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7539, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1380, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7539, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1381, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7538, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1381, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7538, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1381, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7538, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1381, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7538, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1382, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7538, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1382, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7537, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1382, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7537, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1381, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7537, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1382, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7537, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1382, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7536, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1382, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7535, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1382, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7535, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1383, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7535, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1383, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7535, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1383, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7535, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1383, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7535, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1384, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7534, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1384, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7534, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1384, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7534, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1384, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7534, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1385, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7534, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1385, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7533, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1385, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7533, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1385, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7533, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1386, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7533, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1386, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7533, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1386, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7533, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1386, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7532, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1386, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7532, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1387, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7532, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1387, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7532, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1387, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7532, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1387, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7531, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1388, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7531, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1388, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7531, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1388, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7531, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1388, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7531, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1389, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7531, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1389, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7530, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1389, dtype=torch.float64)\n",
      "Objective tensor([[-0.3489]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Penalty tensor([[2.1742e-06]], dtype=torch.float64, grad_fn=<AbsBackward0>)\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "source": [
    "f(x)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PybrVXwMgLuP",
    "outputId": "e2fb4fda-a904-494c-8404-7ca8d649eaf7",
    "ExecuteTime": {
     "end_time": "2025-03-21T07:29:20.459696Z",
     "start_time": "2025-03-21T07:29:20.456779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3491]], dtype=torch.float64, grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sphere Manifold with Exact Penalty and PyGranSO"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:29:24.337610Z",
     "start_time": "2025-03-21T07:29:24.332204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from pygranso.pygranso import pygranso\n",
    "from pygranso.pygransoStruct import pygransoStruct"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:40:35.919947Z",
     "start_time": "2025-03-21T07:40:35.912134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def exact_penalty_with_pygranso(mu_rho, mu_eps):\n",
    "    x = torch.randn(n, 1, dtype=dtype)\n",
    "    # with torch.no_grad():\n",
    "    #     x /= torch.norm(x)\n",
    "    mu = torch.tensor([10.], dtype=dtype)\n",
    "\n",
    "    for iteration in range(1000):\n",
    "        print(\"Iter\", iteration)\n",
    "        \n",
    "        # PyGRANSO\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "        # variables and corresponding dimensions.\n",
    "        var_in = {\"x\": [n, 1]}\n",
    "        \n",
    "        def comb_fn(X_struct):\n",
    "            x = X_struct.x\n",
    "            \n",
    "            # objective function\n",
    "            phi1_x_mu = phi1(x, mu)\n",
    "        \n",
    "            # inequality constraint, matrix form\n",
    "            ci = None\n",
    "        \n",
    "            # equality constraint \n",
    "            ce = None\n",
    "        \n",
    "            return [phi1_x_mu,ci,ce]\n",
    "        \n",
    "        opts = pygransoStruct()\n",
    "        # option for switching QP solver. We only have osqp as the only qp solver in current version. Default is osqp\n",
    "        # opts.QPsolver = 'osqp'\n",
    "        \n",
    "        # set an intial point\n",
    "        # All the user-provided data (vector/matrix/tensor) must be in torch tensor format. \n",
    "        # As PyTorch tensor is single precision by default, one must explicitly set `dtype=torch.double`.\n",
    "        # Also, please make sure the device of provided torch tensor is the same as opts.torch_device.\n",
    "        opts.x0 = x\n",
    "        opts.torch_device = device\n",
    "        opts.print_level = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        soln = pygranso(var_spec = var_in,combined_fn = comb_fn, user_opts = opts)\n",
    "        end = time.time()\n",
    "        print(\"Total Wall Time: {}s\".format(end - start))\n",
    "        x = soln.final.x\n",
    "        \n",
    "        # Exact penalty update\n",
    "        \n",
    "        h = penalty(x)\n",
    "        print(\"Objective\", f(x))\n",
    "        print(\"Penalty\", h)\n",
    "        if h < 1e-5:  # if h(xk ) ≤ τ\n",
    "            break\n",
    "\n",
    "        # Choose new penalty parameter µk+1 > µk ;\n",
    "        if mu * h > mu_eps:\n",
    "            mu *= mu_rho\n",
    "\n",
    "        # Choose new starting point (stay as optimal x1, x2)\n",
    "\n",
    "        print()\n",
    "\n",
    "    return x"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:40:36.877405Z",
     "start_time": "2025-03-21T07:40:36.212083Z"
    }
   },
   "cell_type": "code",
   "source": "x = exact_penalty_with_pygranso(mu_rho=1.1, mu_eps=1e-5)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "Total Wall Time: 0.027605056762695312s\n",
      "Objective tensor([[-2986.1352]], dtype=torch.float64)\n",
      "Penalty tensor([[213.5619]], dtype=torch.float64)\n",
      "\n",
      "Iter 1\n",
      "Total Wall Time: 0.003438711166381836s\n",
      "Objective tensor([[-2986.1352]], dtype=torch.float64)\n",
      "Penalty tensor([[213.5619]], dtype=torch.float64)\n",
      "\n",
      "Iter 2\n",
      "Total Wall Time: 0.0031871795654296875s\n",
      "Objective tensor([[-2986.1352]], dtype=torch.float64)\n",
      "Penalty tensor([[213.5619]], dtype=torch.float64)\n",
      "\n",
      "Iter 3\n",
      "Total Wall Time: 0.0071489810943603516s\n",
      "Objective tensor([[-30426.3588]], dtype=torch.float64)\n",
      "Penalty tensor([[2169.2945]], dtype=torch.float64)\n",
      "\n",
      "Iter 4\n",
      "Total Wall Time: 0.013906002044677734s\n",
      "Objective tensor([[-413723.5897]], dtype=torch.float64)\n",
      "Penalty tensor([[21845.5865]], dtype=torch.float64)\n",
      "\n",
      "Iter 5\n",
      "Total Wall Time: 0.008900165557861328s\n",
      "Objective tensor([[-1449893.3946]], dtype=torch.float64)\n",
      "Penalty tensor([[62666.9699]], dtype=torch.float64)\n",
      "\n",
      "Iter 6\n",
      "Total Wall Time: 0.004840850830078125s\n",
      "Objective tensor([[-1449893.3946]], dtype=torch.float64)\n",
      "Penalty tensor([[62666.9699]], dtype=torch.float64)\n",
      "\n",
      "Iter 7\n",
      "Total Wall Time: 0.002683877944946289s\n",
      "Objective tensor([[-1449893.3946]], dtype=torch.float64)\n",
      "Penalty tensor([[62666.9699]], dtype=torch.float64)\n",
      "\n",
      "Iter 8\n",
      "Total Wall Time: 0.0044558048248291016s\n",
      "Objective tensor([[-5758095.9476]], dtype=torch.float64)\n",
      "Penalty tensor([[256845.1074]], dtype=torch.float64)\n",
      "\n",
      "Iter 9\n",
      "Total Wall Time: 0.013522148132324219s\n",
      "Objective tensor([[-2.1932e+09]], dtype=torch.float64)\n",
      "Penalty tensor([[92915360.0302]], dtype=torch.float64)\n",
      "\n",
      "Iter 10\n",
      "Total Wall Time: 0.5658659934997559s\n",
      "Objective tensor([[-24.2859]], dtype=torch.float64)\n",
      "Penalty tensor([[3.4501e-12]], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "An interesting note: starting with `mu = 1` instead of `mu = 100` yields:\n",
    "```\n",
    "Iter 33\n",
    "Total Wall Time: 0.006292819976806641s\n",
    "Objective tensor([[-1.1599e+50]], dtype=torch.float64)\n",
    "Penalty tensor([[4.9290e+48]], dtype=torch.float64)\n",
    "\n",
    "Iter 34\n",
    "Total Wall Time: 0.9548580646514893s\n",
    "Objective tensor([[-24.2859]], dtype=torch.float64)\n",
    "Penalty tensor([[4.8850e-14]], dtype=torch.float64)\n",
    "```\n",
    "which results in large objectives before the mu value is sufficient."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It also seems that starting with large `mu` may lead to a slightly worse objective (e.g. `mu=100` vs `mu=10000`). This may be because PyGRANSO is better at finding global optima to `f` if the penalty term contributes too much. See https://www.youtube.com/watch?v=7Dvz2HbSyM8 and the textbook about ill conditioning."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Orthogonal RNN"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T07:12:44.854335Z",
     "start_time": "2025-03-22T07:12:44.846949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pygranso.pygranso import pygranso\n",
    "from pygranso.pygransoStruct import pygransoStruct\n",
    "from pygranso.private.getNvar import getNvarTorch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from pygranso.private.getObjGrad import getObjGradDL"
   ],
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T07:26:55.751340Z",
     "start_time": "2025-03-22T07:26:53.669170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "torch.set_default_device(device)\n",
    "torch.set_default_dtype(dtype)\n",
    "\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 30\n",
    "num_layers = 1\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "double_precision = torch.double\n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.reshape(x,(batch_size,sequence_length,input_size))\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device=device, dtype=double_precision)\n",
    "        out, hidden = self.rnn(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device=device, dtype=double_precision)\n",
    "model.train()\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root = './examples/data/mnist',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True,\n",
    ")\n",
    "\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(train_data,\n",
    "                                        batch_size=100,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=1),\n",
    "}\n",
    "\n",
    "inputs, labels = next(iter(loaders['train']))\n",
    "inputs, labels = inputs.reshape(-1, sequence_length, input_size).to(device=device, dtype=double_precision), labels.to(device=device)"
   ],
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T07:26:55.757413Z",
     "start_time": "2025-03-22T07:26:55.754001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def f(model, inputs, labels):\n",
    "    logits = model(inputs)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    return loss_fn(logits, labels)\n",
    "\n",
    "def penalty(model):\n",
    "    A = list(model.parameters())[1]\n",
    "    \n",
    "    # print(A)\n",
    "    \n",
    "    return torch.norm(A.T @ A - torch.eye(hidden_size), p=1)\n",
    "\n",
    "def phi1(model, mu):\n",
    "    return f(model, inputs, labels) + mu * penalty(model)"
   ],
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T07:26:55.762227Z",
     "start_time": "2025-03-22T07:26:55.758552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def exact_penalty_with_pygranso(mu_rho, mu_eps):\n",
    "    mu = torch.tensor([1.], dtype=dtype)\n",
    "\n",
    "    for iteration in range(1000):\n",
    "        print(\"Iter\", iteration)\n",
    "        \n",
    "        # PyGRANSO\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "        def comb_fn(model):\n",
    "            # objective function\n",
    "            phi1_x_mu = phi1(model, mu)\n",
    "        \n",
    "            # inequality constraint, matrix form\n",
    "            ci = None\n",
    "        \n",
    "            # equality constraint \n",
    "            ce = None\n",
    "        \n",
    "            return [phi1_x_mu,ci,ce]\n",
    "        \n",
    "        opts = pygransoStruct()\n",
    "        # option for switching QP solver. We only have osqp as the only qp solver in current version. Default is osqp\n",
    "        # opts.QPsolver = 'osqp'\n",
    "        \n",
    "        # set an intial point\n",
    "        # All the user-provided data (vector/matrix/tensor) must be in torch tensor format. \n",
    "        # As PyTorch tensor is single precision by default, one must explicitly set `dtype=torch.double`.\n",
    "        # Also, please make sure the device of provided torch tensor is the same as opts.torch_device.\n",
    "        nvar = getNvarTorch(model.parameters())\n",
    "        opts.x0 = torch.nn.utils.parameters_to_vector(model.parameters()).detach().reshape(nvar,1)\n",
    "        opts.torch_device = device\n",
    "        opts.opt_tol = 1e-5\n",
    "        opts.viol_eq_tol = 1e-5\n",
    "        opts.print_level = 1\n",
    "        opts.print_frequency = 50\n",
    "        \n",
    "        # opts.maxit = 1000 yields 80% acc but seems far from reaching stationarity\n",
    "        #  900 ║  - │   -   ║  0.76603445858 ║   -  │   -  ║ QN │     6 │ 0.031250 ║     1 │ 0.246480   ║ \n",
    "        #  950 ║  - │   -   ║  0.70312720877 ║   -  │   -  ║ QN │     6 │ 0.031250 ║     1 │ 0.471985   ║ \n",
    "        # ═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
    "        #      ║ Penalty Fn ║                ║  Violation  ║ <--- Line Search ---> ║ <- Stationarity -> ║ \n",
    "        # Iter ║ Mu │ Value ║    Objective   ║ Ineq │  Eq  ║ SD │ Evals │     t    ║ Grads │    Value   ║ \n",
    "        # ═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
    "        # 1000 ║  - │   -   ║  0.62800096715 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.430504   ║ \n",
    "        opts.maxit = 3000\n",
    "        \n",
    "        start = time.time()\n",
    "        soln = pygranso(var_spec = model,combined_fn = comb_fn, user_opts = opts)\n",
    "        end = time.time()\n",
    "        print(\"Total Wall Time: {}s\".format(end - start))\n",
    "        torch.nn.utils.vector_to_parameters(soln.final.x, model.parameters())\n",
    "        \n",
    "        # Exact penalty update\n",
    "        \n",
    "        h = penalty(model)\n",
    "        print(\"Objective:\", f(model, inputs, labels))\n",
    "        print(\"Penalty:\", h)\n",
    "        if h < 1e-3:  # if h(xk ) ≤ τ\n",
    "            break\n",
    "\n",
    "        # Choose new penalty parameter µk+1 > µk ;\n",
    "        if mu * h > mu_eps:\n",
    "            mu *= mu_rho\n",
    "\n",
    "        # Choose new starting point (stay as optimal x1, x2)\n",
    "\n",
    "        print()\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T07:30:03.701570Z",
     "start_time": "2025-03-22T07:26:55.764285Z"
    }
   },
   "cell_type": "code",
   "source": "model = exact_penalty_with_pygranso(mu_rho=1.1, mu_eps=1e-5)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "\n",
      "\n",
      "\u001B[33m╔═════ QP SOLVER NOTICE ════════════════════════════════════════════════════════════════════════╗\n",
      "\u001B[0m\u001B[33m║  PyGRANSO requires a quadratic program (QP) solver that has a quadprog-compatible interface,  ║\n",
      "\u001B[0m\u001B[33m║  the default is osqp. Users may provide their own wrapper for the QP solver.                  ║\n",
      "\u001B[0m\u001B[33m║  To disable this notice, set opts.quadprog_info_msg = False                                   ║\n",
      "\u001B[0m\u001B[33m╚═══════════════════════════════════════════════════════════════════════════════════════════════╝\n",
      "\u001B[0m══════════════════════════════════════════════════════════════════════════════════════════════╗\n",
      "PyGRANSO: A PyTorch-enabled port of GRANSO with auto-differentiation                          ║ \n",
      "Version 1.2.0                                                                                 ║ \n",
      "Licensed under the AGPLv3, Copyright (C) 2021-2022 Tim Mitchell and Buyun Liang               ║ \n",
      "══════════════════════════════════════════════════════════════════════════════════════════════╣\n",
      "Problem specifications:                                                                       ║ \n",
      " # of variables                     :   2110                                                  ║ \n",
      " # of inequality constraints        :      0                                                  ║ \n",
      " # of equality constraints          :      0                                                  ║ \n",
      "═════╦════════════╦════════════════╦═════════════╦═══════════════════════╦════════════════════╣\n",
      "     ║ Penalty Fn ║                ║  Violation  ║ <--- Line Search ---> ║ <- Stationarity -> ║ \n",
      "Iter ║ Mu │ Value ║    Objective   ║ Ineq │  Eq  ║ SD │ Evals │     t    ║ Grads │    Value   ║ \n",
      "═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
      "   0 ║  - │   -   ║  65.5307665692 ║   -  │   -  ║ -  │     1 │ 0.000000 ║     1 │ 41.55537   ║ \n",
      "  50 ║  - │   -   ║  20.5509094175 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 1.888499   ║ \n",
      " 100 ║  - │   -   ║  7.02452702911 ║   -  │   -  ║ QN │     7 │ 0.015625 ║     1 │ 3.290875   ║ \n",
      " 150 ║  - │   -   ║  4.08288824014 ║   -  │   -  ║ QN │     9 │ 0.003906 ║     1 │ 1.526841   ║ \n",
      " 200 ║  - │   -   ║  3.22653394069 ║   -  │   -  ║ QN │    10 │ 0.001953 ║     1 │ 1.892682   ║ \n",
      " 250 ║  - │   -   ║  2.84832067264 ║   -  │   -  ║ QN │    10 │ 0.001953 ║     1 │ 1.287794   ║ \n",
      " 300 ║  - │   -   ║  2.63801550350 ║   -  │   -  ║ QN │    11 │ 9.77e-04 ║     1 │ 1.089907   ║ \n",
      " 350 ║  - │   -   ║  2.48431235919 ║   -  │   -  ║ QN │    13 │ 2.44e-04 ║     1 │ 0.810224   ║ \n",
      " 400 ║  - │   -   ║  2.39410925024 ║   -  │   -  ║ QN │    11 │ 9.77e-04 ║     1 │ 0.922969   ║ \n",
      " 450 ║  - │   -   ║  2.33490714724 ║   -  │   -  ║ QN │    11 │ 9.77e-04 ║     1 │ 0.527912   ║ \n",
      " 500 ║  - │   -   ║  2.28973333136 ║   -  │   -  ║ QN │    11 │ 9.77e-04 ║     1 │ 0.487076   ║ \n",
      " 550 ║  - │   -   ║  2.24477364545 ║   -  │   -  ║ QN │    10 │ 0.001953 ║     1 │ 0.241375   ║ \n",
      " 600 ║  - │   -   ║  2.20244269316 ║   -  │   -  ║ QN │     8 │ 0.007812 ║     1 │ 0.083755   ║ \n",
      " 650 ║  - │   -   ║  2.00039541281 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.109074   ║ \n",
      " 700 ║  - │   -   ║  1.60451149876 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.217035   ║ \n",
      " 750 ║  - │   -   ║  1.18620699787 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.124510   ║ \n",
      " 800 ║  - │   -   ║  0.98559269805 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.237203   ║ \n",
      " 850 ║  - │   -   ║  0.84173505360 ║   -  │   -  ║ QN │     6 │ 0.031250 ║     1 │ 0.374834   ║ \n",
      " 900 ║  - │   -   ║  0.76603445858 ║   -  │   -  ║ QN │     6 │ 0.031250 ║     1 │ 0.246480   ║ \n",
      " 950 ║  - │   -   ║  0.70312720877 ║   -  │   -  ║ QN │     6 │ 0.031250 ║     1 │ 0.471985   ║ \n",
      "═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
      "     ║ Penalty Fn ║                ║  Violation  ║ <--- Line Search ---> ║ <- Stationarity -> ║ \n",
      "Iter ║ Mu │ Value ║    Objective   ║ Ineq │  Eq  ║ SD │ Evals │     t    ║ Grads │    Value   ║ \n",
      "═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
      "1000 ║  - │   -   ║  0.62800096715 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.430504   ║ \n",
      "1050 ║  - │   -   ║  0.54800567027 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.446421   ║ \n",
      "1100 ║  - │   -   ║  0.45846285126 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 1.192969   ║ \n",
      "1150 ║  - │   -   ║  0.39708272478 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.232815   ║ \n",
      "1200 ║  - │   -   ║  0.34317760688 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.357358   ║ \n",
      "1250 ║  - │   -   ║  0.27429936478 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.060567   ║ \n",
      "1300 ║  - │   -   ║  0.22447503924 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 1.276172   ║ \n",
      "1350 ║  - │   -   ║  0.17908206341 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.256132   ║ \n",
      "1400 ║  - │   -   ║  0.13136752152 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.368413   ║ \n",
      "1450 ║  - │   -   ║  0.10478679986 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.448212   ║ \n",
      "1500 ║  - │   -   ║  0.08340522401 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.189956   ║ \n",
      "1550 ║  - │   -   ║  0.05977313310 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.228708   ║ \n",
      "1600 ║  - │   -   ║  0.04966533726 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.307896   ║ \n",
      "1650 ║  - │   -   ║  0.03842641824 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.222193   ║ \n",
      "1700 ║  - │   -   ║  0.03239308508 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.075488   ║ \n",
      "1750 ║  - │   -   ║  0.02521819824 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.024152   ║ \n",
      "1800 ║  - │   -   ║  0.01949525590 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.127160   ║ \n",
      "1850 ║  - │   -   ║  0.01396338814 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.015541   ║ \n",
      "1900 ║  - │   -   ║  0.01159794313 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.412510   ║ \n",
      "1950 ║  - │   -   ║  0.00981670183 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.053296   ║ \n",
      "═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
      "     ║ Penalty Fn ║                ║  Violation  ║ <--- Line Search ---> ║ <- Stationarity -> ║ \n",
      "Iter ║ Mu │ Value ║    Objective   ║ Ineq │  Eq  ║ SD │ Evals │     t    ║ Grads │    Value   ║ \n",
      "═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
      "2000 ║  - │   -   ║  0.00850264181 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.382627   ║ \n",
      "2050 ║  - │   -   ║  0.00723513075 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.104520   ║ \n",
      "2100 ║  - │   -   ║  0.00655266998 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.015621   ║ \n",
      "2150 ║  - │   -   ║  0.00565031213 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.023964   ║ \n",
      "2200 ║  - │   -   ║  0.00505685748 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.033169   ║ \n",
      "2250 ║  - │   -   ║  0.00452577692 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.103228   ║ \n",
      "2300 ║  - │   -   ║  0.00375359612 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.097419   ║ \n",
      "2350 ║  - │   -   ║  0.00318004836 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.008915   ║ \n",
      "2400 ║  - │   -   ║  0.00303600392 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.122150   ║ \n",
      "2450 ║  - │   -   ║  0.00277793713 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.015143   ║ \n",
      "2500 ║  - │   -   ║  0.00245842516 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.027354   ║ \n",
      "2550 ║  - │   -   ║  0.00201475657 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.016733   ║ \n",
      "2600 ║  - │   -   ║  0.00195118711 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.046096   ║ \n",
      "2650 ║  - │   -   ║  0.00173960602 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.020170   ║ \n",
      "2700 ║  - │   -   ║  0.00154357132 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.035979   ║ \n",
      "2750 ║  - │   -   ║  0.00141364455 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.017463   ║ \n",
      "2800 ║  - │   -   ║  0.00131458912 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.022784   ║ \n",
      "2850 ║  - │   -   ║  0.00123666826 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.030122   ║ \n",
      "2900 ║  - │   -   ║  0.00118519855 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.010612   ║ \n",
      "2950 ║  - │   -   ║  0.00109702984 ║   -  │   -  ║ QN │     2 │ 0.500000 ║     1 │ 0.022491   ║ \n",
      "═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
      "     ║ Penalty Fn ║                ║  Violation  ║ <--- Line Search ---> ║ <- Stationarity -> ║ \n",
      "Iter ║ Mu │ Value ║    Objective   ║ Ineq │  Eq  ║ SD │ Evals │     t    ║ Grads │    Value   ║ \n",
      "═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
      "3000 ║  - │   -   ║  9.3465700e-04 ║   -  │   -  ║ QN │     2 │ 0.500000 ║     1 │ 0.008833   ║ \n",
      "═════╩════════════╩════════════════╩═════════════╩═══════════════════════╩════════════════════╣\n",
      "F = final iterate, B = Best (to tolerance), MF = Most Feasible                                ║ \n",
      "Optimization results:                                                                         ║ \n",
      "═════╦════════════╦════════════════╦═════════════╦═══════════════════════╦════════════════════╣\n",
      "   F ║    │       ║  9.3465700e-04 ║   -  │   -  ║    │       │          ║       │            ║ \n",
      "   B ║    │       ║  9.3465700e-04 ║   -  │   -  ║    │       │          ║       │            ║ \n",
      "═════╩════════════╩════════════════╩═════════════╩═══════════════════════╩════════════════════╣\n",
      "Iterations:              3000                                                                 ║ \n",
      "Function evaluations:    16058                                                                ║ \n",
      "PyGRANSO termination code: 4 --- max iterations reached.                                      ║ \n",
      "══════════════════════════════════════════════════════════════════════════════════════════════╝\n",
      "Total Wall Time: 187.92979216575623s\n",
      "Objective: tensor(0.0009, grad_fn=<NllLossBackward0>)\n",
      "Penalty: tensor(2.1675e-05, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T07:30:03.714710Z",
     "start_time": "2025-03-22T07:30:03.704022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logits = model(inputs)\n",
    "_, predicted = torch.max(logits.data, 1)\n",
    "correct = (predicted == labels).sum().item()\n",
    "print(\"Final acc = {:.2f}%\".format((100 * correct/len(inputs))))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final acc = 100.00%\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
