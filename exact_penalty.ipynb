{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# fix the random seed\n",
    "torch.manual_seed(55272025)\n",
    "\n",
    "dtype = torch.float\n",
    "w = 8"
   ],
   "metadata": {
    "id": "BSRr9krmAuB5",
    "ExecuteTime": {
     "end_time": "2025-03-27T18:18:41.730643Z",
     "start_time": "2025-03-27T18:18:28.568100Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rosenbrock"
   ],
   "metadata": {
    "id": "miojWLbdfRUt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Rosenbrock with equality constraint\n",
    "def f(x1, x2):\n",
    "    return w * torch.abs(x1 ** 2 - x2) + (1 - x1) ** 2\n",
    "\n",
    "def penalty(x1, x2):\n",
    "    return torch.abs(math.sqrt(2) * x1 - 1) + torch.abs(2 * x2 - 1)\n",
    "\n",
    "def phi1(x1, x2, mu):\n",
    "    return f(x1, x2) + mu * penalty(x1, x2)"
   ],
   "metadata": {
    "id": "Tma-roqGAXBE",
    "ExecuteTime": {
     "end_time": "2025-03-27T18:18:41.753541Z",
     "start_time": "2025-03-27T18:18:41.733781Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ypyyvqq4AJIQ",
    "ExecuteTime": {
     "end_time": "2025-03-27T18:18:41.766560Z",
     "start_time": "2025-03-27T18:18:41.755291Z"
    }
   },
   "source": [
    "def backtracking_line_search(t_rho, c, x_eps, mu_rho, mu_eps):\n",
    "    x1 = torch.randn(1, 1, dtype=dtype, requires_grad=True)\n",
    "    x2 = torch.randn(1, 1, dtype=dtype, requires_grad=True)\n",
    "    mu = torch.tensor([1.], dtype=dtype)\n",
    "\n",
    "    for iteration in range(1000):\n",
    "        print(\"Iter\", iteration)\n",
    "\n",
    "        # Find an approximate minimizer xk of phi1(x; µk), starting at xk^s\n",
    "        for epoch in range(100):\n",
    "            phi1_x_mu = phi1(x1, x2, mu)\n",
    "            phi1_x_mu.backward()\n",
    "\n",
    "            # if epoch % 100 == 99:\n",
    "            print(\"Epoch\", epoch)\n",
    "            print(\"Objective + penalty:\", phi1_x_mu.item())\n",
    "            print(\"Vars:\", x1, x2, mu)\n",
    "            print(\"Grads:\", torch.norm(x1.grad), torch.norm(x2.grad))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if torch.norm(x1.grad) ** 2 + torch.norm(x2.grad) ** 2 <= x_eps:\n",
    "                    break\n",
    "\n",
    "                t = 1\n",
    "                while t >= 1e-8 and phi1(x1 - t * x1.grad, x2 - t * x2.grad, mu) - phi1_x_mu >= -c * t * (torch.norm(x1.grad) ** 2 + torch.norm(x2.grad) ** 2):\n",
    "                    t *= t_rho\n",
    "                print(\"t:\", t)\n",
    "                if t < 1e-8:\n",
    "                    break\n",
    "\n",
    "                x1 -= t * x1.grad\n",
    "                x1.grad = None\n",
    "                x2 -= t * x2.grad\n",
    "                x2.grad = None\n",
    "\n",
    "        h = penalty(x1, x2)\n",
    "        print(\"Penalty\", h)\n",
    "        if h < 1e-5:  # if h(xk ) ≤ τ\n",
    "            break\n",
    "\n",
    "        # Choose new penalty parameter µk+1 > µk ;\n",
    "        if mu * h > mu_eps:\n",
    "            mu *= mu_rho\n",
    "\n",
    "        # Choose new starting point (stay as optimal x1, x2)\n",
    "\n",
    "        print()\n",
    "\n",
    "    return x1, x2"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "backtracking_line_search(t_rho=0.8, c=0, x_eps=1e-5, mu_rho=1.1, mu_eps=1e-5)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmPMnY1ICSsR",
    "outputId": "14608986-66e7-4505-ea11-67b48a7ce51c",
    "ExecuteTime": {
     "end_time": "2025-03-27T18:18:45.397201Z",
     "start_time": "2025-03-27T18:18:41.772664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "Epoch 0\n",
      "Objective + penalty: 9.709888458251953\n",
      "Vars: tensor([[-1.0277]], requires_grad=True) tensor([[0.7176]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(21.9136) tensor(6.)\n",
      "t: 0.10737418240000006\n",
      "Epoch 1\n",
      "Objective + penalty: 5.858539581298828\n",
      "Vars: tensor([[1.3252]], requires_grad=True) tensor([[1.3618]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(23.2680) tensor(6.)\n",
      "t: 0.011529215046068483\n",
      "Epoch 2\n",
      "Objective + penalty: 4.8707075119018555\n",
      "Vars: tensor([[1.0570]], requires_grad=True) tensor([[1.4310]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(15.3831) tensor(10.)\n",
      "t: 0.011529215046068483\n",
      "Epoch 3\n",
      "Objective + penalty: 4.094383239746094\n",
      "Vars: tensor([[1.2343]], requires_grad=True) tensor([[1.3157]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(21.6317) tensor(6.)\n",
      "t: 0.00737869762948383\n",
      "Epoch 4\n",
      "Objective + penalty: 3.8853907585144043\n",
      "Vars: tensor([[1.0747]], requires_grad=True) tensor([[1.3600]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(15.6315) tensor(10.)\n",
      "t: 0.00737869762948383\n",
      "Epoch 5\n",
      "Objective + penalty: 3.3313918113708496\n",
      "Vars: tensor([[1.1900]], requires_grad=True) tensor([[1.2862]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(20.8348) tensor(6.)\n",
      "t: 0.004722366482869652\n",
      "Epoch 6\n",
      "Objective + penalty: 3.163892984390259\n",
      "Vars: tensor([[1.0916]], requires_grad=True) tensor([[1.3145]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(15.8688) tensor(10.)\n",
      "t: 0.004722366482869652\n",
      "Epoch 7\n",
      "Objective + penalty: 2.9610753059387207\n",
      "Vars: tensor([[1.1666]], requires_grad=True) tensor([[1.2673]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(20.4127) tensor(6.)\n",
      "t: 0.0037778931862957215\n",
      "Epoch 8\n",
      "Objective + penalty: 2.9528698921203613\n",
      "Vars: tensor([[1.0895]], requires_grad=True) tensor([[1.2900]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(15.8383) tensor(10.)\n",
      "t: 0.0037778931862957215\n",
      "Epoch 9\n",
      "Objective + penalty: 2.7016706466674805\n",
      "Vars: tensor([[1.1493]], requires_grad=True) tensor([[1.2522]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(20.1016) tensor(6.)\n",
      "t: 0.002417851639229262\n",
      "Epoch 10\n",
      "Objective + penalty: 2.5413622856140137\n",
      "Vars: tensor([[1.1007]], requires_grad=True) tensor([[1.2667]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(15.9955) tensor(10.)\n",
      "t: 0.0019342813113834097\n",
      "Epoch 11\n",
      "Objective + penalty: 2.3784401416778564\n",
      "Vars: tensor([[1.1316]], requires_grad=True) tensor([[1.2473]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.7837) tensor(6.)\n",
      "t: 0.0012379400392853823\n",
      "Epoch 12\n",
      "Objective + penalty: 2.318767786026001\n",
      "Vars: tensor([[1.1071]], requires_grad=True) tensor([[1.2548]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.0858) tensor(10.)\n",
      "t: 0.0012379400392853823\n",
      "Epoch 13\n",
      "Objective + penalty: 2.317777395248413\n",
      "Vars: tensor([[1.1271]], requires_grad=True) tensor([[1.2424]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.7013) tensor(6.)\n",
      "t: 0.0009903520314283058\n",
      "Epoch 14\n",
      "Objective + penalty: 2.2479259967803955\n",
      "Vars: tensor([[1.1075]], requires_grad=True) tensor([[1.2483]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.0914) tensor(10.)\n",
      "t: 0.0007922816251426447\n",
      "Epoch 15\n",
      "Objective + penalty: 2.1968472003936768\n",
      "Vars: tensor([[1.1203]], requires_grad=True) tensor([[1.2404]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.5795) tensor(6.)\n",
      "t: 0.0005070602400912927\n",
      "Epoch 16\n",
      "Objective + penalty: 2.153671979904175\n",
      "Vars: tensor([[1.1104]], requires_grad=True) tensor([[1.2435]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1309) tensor(10.)\n",
      "t: 0.00040564819207303417\n",
      "Epoch 17\n",
      "Objective + penalty: 2.1367690563201904\n",
      "Vars: tensor([[1.1169]], requires_grad=True) tensor([[1.2394]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.5186) tensor(6.)\n",
      "t: 0.00032451855365842736\n",
      "Epoch 18\n",
      "Objective + penalty: 2.129202127456665\n",
      "Vars: tensor([[1.1106]], requires_grad=True) tensor([[1.2413]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1339) tensor(10.)\n",
      "t: 0.00032451855365842736\n",
      "Epoch 19\n",
      "Objective + penalty: 2.123128652572632\n",
      "Vars: tensor([[1.1158]], requires_grad=True) tensor([[1.2381]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4988) tensor(6.)\n",
      "t: 0.0002596148429267419\n",
      "Epoch 20\n",
      "Objective + penalty: 2.1095266342163086\n",
      "Vars: tensor([[1.1108]], requires_grad=True) tensor([[1.2397]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1363) tensor(10.)\n",
      "t: 0.00020769187434139353\n",
      "Epoch 21\n",
      "Objective + penalty: 2.092910051345825\n",
      "Vars: tensor([[1.1141]], requires_grad=True) tensor([[1.2376]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4681) tensor(6.)\n",
      "t: 0.00013292279957849188\n",
      "Epoch 22\n",
      "Objective + penalty: 2.0844080448150635\n",
      "Vars: tensor([[1.1115]], requires_grad=True) tensor([[1.2384]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1470) tensor(10.)\n",
      "t: 0.0001063382396627935\n",
      "Epoch 23\n",
      "Objective + penalty: 2.0775606632232666\n",
      "Vars: tensor([[1.1132]], requires_grad=True) tensor([[1.2373]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4524) tensor(6.)\n",
      "t: 6.805647338418785e-05\n",
      "Epoch 24\n",
      "Objective + penalty: 2.0715060234069824\n",
      "Vars: tensor([[1.1119]], requires_grad=True) tensor([[1.2377]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1525) tensor(10.)\n",
      "t: 5.444517870735028e-05\n",
      "Epoch 25\n",
      "Objective + penalty: 2.0697338581085205\n",
      "Vars: tensor([[1.1128]], requires_grad=True) tensor([[1.2372]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4444) tensor(6.)\n",
      "t: 4.3556142965880224e-05\n",
      "Epoch 26\n",
      "Objective + penalty: 2.0681493282318115\n",
      "Vars: tensor([[1.1119]], requires_grad=True) tensor([[1.2374]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1529) tensor(10.)\n",
      "t: 4.3556142965880224e-05\n",
      "Epoch 27\n",
      "Objective + penalty: 2.0679945945739746\n",
      "Vars: tensor([[1.1126]], requires_grad=True) tensor([[1.2370]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4418) tensor(6.)\n",
      "t: 3.484491437270418e-05\n",
      "Epoch 28\n",
      "Objective + penalty: 2.065463066101074\n",
      "Vars: tensor([[1.1120]], requires_grad=True) tensor([[1.2372]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1533) tensor(10.)\n",
      "t: 2.7875931498163346e-05\n",
      "Epoch 29\n",
      "Objective + penalty: 2.063995122909546\n",
      "Vars: tensor([[1.1124]], requires_grad=True) tensor([[1.2369]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4377) tensor(6.)\n",
      "t: 2.2300745198530677e-05\n",
      "Epoch 30\n",
      "Objective + penalty: 2.0637412071228027\n",
      "Vars: tensor([[1.1120]], requires_grad=True) tensor([[1.2371]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1535) tensor(10.)\n",
      "t: 2.2300745198530677e-05\n",
      "Epoch 31\n",
      "Objective + penalty: 2.063108205795288\n",
      "Vars: tensor([[1.1123]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4364) tensor(6.)\n",
      "t: 1.784059615882454e-05\n",
      "Epoch 32\n",
      "Objective + penalty: 2.062364101409912\n",
      "Vars: tensor([[1.1120]], requires_grad=True) tensor([[1.2370]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1537) tensor(10.)\n",
      "t: 1.4272476927059634e-05\n",
      "Epoch 33\n",
      "Objective + penalty: 2.061063051223755\n",
      "Vars: tensor([[1.1122]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4343) tensor(6.)\n",
      "t: 9.134385233318167e-06\n",
      "Epoch 34\n",
      "Objective + penalty: 2.0606284141540527\n",
      "Vars: tensor([[1.1120]], requires_grad=True) tensor([[1.2369]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1545) tensor(10.)\n",
      "t: 7.307508186654534e-06\n",
      "Epoch 35\n",
      "Objective + penalty: 2.060016632080078\n",
      "Vars: tensor([[1.1122]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4332) tensor(6.)\n",
      "t: 4.676805239458903e-06\n",
      "Epoch 36\n",
      "Objective + penalty: 2.0597386360168457\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1549) tensor(10.)\n",
      "t: 3.7414441915671226e-06\n",
      "Epoch 37\n",
      "Objective + penalty: 2.059481620788574\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4327) tensor(6.)\n",
      "t: 2.394524282602959e-06\n",
      "Epoch 38\n",
      "Objective + penalty: 2.0592832565307617\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 1.9156194260823675e-06\n",
      "Epoch 39\n",
      "Objective + penalty: 2.0592093467712402\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4324) tensor(6.)\n",
      "t: 1.5324955408658941e-06\n",
      "Epoch 40\n",
      "Objective + penalty: 2.059164524078369\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 1.5324955408658941e-06\n",
      "Epoch 41\n",
      "Objective + penalty: 2.0591495037078857\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4323) tensor(6.)\n",
      "t: 1.2259964326927154e-06\n",
      "Epoch 42\n",
      "Objective + penalty: 2.0590689182281494\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 9.807971461541723e-07\n",
      "Epoch 43\n",
      "Objective + penalty: 2.0590083599090576\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4322) tensor(6.)\n",
      "t: 7.846377169233379e-07\n",
      "Epoch 44\n",
      "Objective + penalty: 2.0590081214904785\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 7.846377169233379e-07\n",
      "Epoch 45\n",
      "Objective + penalty: 2.058976888656616\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4321) tensor(6.)\n",
      "t: 6.277101735386704e-07\n",
      "Epoch 46\n",
      "Objective + penalty: 2.0589599609375\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 6.277101735386704e-07\n",
      "Epoch 47\n",
      "Objective + penalty: 2.0589523315429688\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4321) tensor(6.)\n",
      "t: 5.021681388309363e-07\n",
      "Epoch 48\n",
      "Objective + penalty: 2.058920383453369\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 4.017345110647491e-07\n",
      "Epoch 49\n",
      "Objective + penalty: 2.058894157409668\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4320) tensor(6.)\n",
      "t: 2.5711008708143947e-07\n",
      "Epoch 50\n",
      "Objective + penalty: 2.058872938156128\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 2.056880696651516e-07\n",
      "Epoch 51\n",
      "Objective + penalty: 2.0588645935058594\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4320) tensor(6.)\n",
      "t: 1.645504557321213e-07\n",
      "Epoch 52\n",
      "Objective + penalty: 2.058859348297119\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 1.645504557321213e-07\n",
      "Epoch 53\n",
      "Objective + penalty: 2.0588574409484863\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4320) tensor(6.)\n",
      "t: 1.3164036458569703e-07\n",
      "Epoch 54\n",
      "Objective + penalty: 2.058849334716797\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 1.0531229166855763e-07\n",
      "Epoch 55\n",
      "Objective + penalty: 2.058842420578003\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4320) tensor(6.)\n",
      "t: 6.739986666787689e-08\n",
      "Epoch 56\n",
      "Objective + penalty: 2.0588362216949463\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 5.3919893334301516e-08\n",
      "Epoch 57\n",
      "Objective + penalty: 2.0588345527648926\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4320) tensor(6.)\n",
      "t: 4.313591466744121e-08\n",
      "Epoch 58\n",
      "Objective + penalty: 2.0588326454162598\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 3.450873173395297e-08\n",
      "Epoch 59\n",
      "Objective + penalty: 2.058830738067627\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4320) tensor(6.)\n",
      "t: 2.7606985387162378e-08\n",
      "Epoch 60\n",
      "Objective + penalty: 2.0588302612304688\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 2.7606985387162378e-08\n",
      "Epoch 61\n",
      "Objective + penalty: 2.058828353881836\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4320) tensor(6.)\n",
      "t: 1.7668470647783922e-08\n",
      "Epoch 62\n",
      "Objective + penalty: 2.0588274002075195\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(16.1551) tensor(10.)\n",
      "t: 1.7668470647783922e-08\n",
      "Epoch 63\n",
      "Objective + penalty: 2.058826446533203\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.])\n",
      "Grads: tensor(19.4320) tensor(6.)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 1\n",
      "Epoch 0\n",
      "Objective + penalty: 2.2634522914886475\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.1000])\n",
      "Grads: tensor(39.0053) tensor(11.8000)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 2\n",
      "Epoch 0\n",
      "Objective + penalty: 2.4885408878326416\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.2100])\n",
      "Grads: tensor(58.7343) tensor(17.3800)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 3\n",
      "Epoch 0\n",
      "Objective + penalty: 2.736138343811035\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.3310])\n",
      "Grads: tensor(78.6343) tensor(22.7180)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 4\n",
      "Epoch 0\n",
      "Objective + penalty: 3.008495569229126\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.4641])\n",
      "Grads: tensor(98.7226) tensor(27.7898)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 5\n",
      "Epoch 0\n",
      "Objective + penalty: 3.3080883026123047\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.6105])\n",
      "Grads: tensor(119.0179) tensor(32.5688)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 6\n",
      "Epoch 0\n",
      "Objective + penalty: 3.6376402378082275\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.7716])\n",
      "Grads: tensor(139.5410) tensor(37.0257)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 7\n",
      "Epoch 0\n",
      "Objective + penalty: 4.000147819519043\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([1.9487])\n",
      "Grads: tensor(160.3147) tensor(41.1282)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 8\n",
      "Epoch 0\n",
      "Objective + penalty: 4.3989057540893555\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([2.1436])\n",
      "Grads: tensor(181.3639) tensor(44.8410)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 9\n",
      "Epoch 0\n",
      "Objective + penalty: 4.8375396728515625\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([2.3579])\n",
      "Grads: tensor(202.7163) tensor(48.1251)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 10\n",
      "Epoch 0\n",
      "Objective + penalty: 5.320036888122559\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([2.5937])\n",
      "Grads: tensor(224.4021) tensor(50.9377)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 11\n",
      "Epoch 0\n",
      "Objective + penalty: 5.8507843017578125\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([2.8531])\n",
      "Grads: tensor(246.4548) tensor(53.2314)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 12\n",
      "Epoch 0\n",
      "Objective + penalty: 6.434605598449707\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([3.1384])\n",
      "Grads: tensor(268.9109) tensor(54.9546)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 13\n",
      "Epoch 0\n",
      "Objective + penalty: 7.076809883117676\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([3.4523])\n",
      "Grads: tensor(291.8109) tensor(56.0500)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 14\n",
      "Epoch 0\n",
      "Objective + penalty: 7.783234119415283\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([3.7975])\n",
      "Grads: tensor(315.1992) tensor(56.4550)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 15\n",
      "Epoch 0\n",
      "Objective + penalty: 8.560300827026367\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([4.1772])\n",
      "Grads: tensor(339.1244) tensor(56.1005)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 16\n",
      "Epoch 0\n",
      "Objective + penalty: 9.415074348449707\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([4.5950])\n",
      "Grads: tensor(363.6404) tensor(54.9106)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 17\n",
      "Epoch 0\n",
      "Objective + penalty: 10.355324745178223\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([5.0545])\n",
      "Grads: tensor(388.8063) tensor(52.8016)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 18\n",
      "Epoch 0\n",
      "Objective + penalty: 11.38960075378418\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([5.5599])\n",
      "Grads: tensor(414.6869) tensor(49.6818)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 19\n",
      "Epoch 0\n",
      "Objective + penalty: 12.527304649353027\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([6.1159])\n",
      "Grads: tensor(441.3539) tensor(45.4500)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 20\n",
      "Epoch 0\n",
      "Objective + penalty: 13.778779029846191\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([6.7275])\n",
      "Grads: tensor(468.8857) tensor(39.9950)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 21\n",
      "Epoch 0\n",
      "Objective + penalty: 15.155401229858398\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([7.4003])\n",
      "Grads: tensor(497.3690) tensor(33.1945)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 22\n",
      "Epoch 0\n",
      "Objective + penalty: 16.6696834564209\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([8.1403])\n",
      "Grads: tensor(526.8988) tensor(24.9139)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 23\n",
      "Epoch 0\n",
      "Objective + penalty: 18.33539581298828\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([8.9543])\n",
      "Grads: tensor(557.5798) tensor(15.0053)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 24\n",
      "Epoch 0\n",
      "Objective + penalty: 20.167678833007812\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([9.8497])\n",
      "Grads: tensor(589.5272) tensor(3.3058)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[2.0463]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Iter 25\n",
      "Epoch 0\n",
      "Objective + penalty: 22.183189392089844\n",
      "Vars: tensor([[1.1121]], requires_grad=True) tensor([[1.2368]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(622.8676) tensor(10.3636)\n",
      "t: 0.0006338253001141158\n",
      "Epoch 1\n",
      "Objective + penalty: 21.784282684326172\n",
      "Vars: tensor([[0.7173]], requires_grad=True) tensor([[1.2302]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.2803) tensor(29.6694)\n",
      "t: 0.043980465111040035\n",
      "Epoch 2\n",
      "Objective + penalty: 17.914127349853516\n",
      "Vars: tensor([[0.5730]], requires_grad=True) tensor([[-0.0747]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(7.0079) tensor(29.6694)\n",
      "t: 0.03518437208883203\n",
      "Epoch 3\n",
      "Objective + penalty: 14.303622245788574\n",
      "Vars: tensor([[0.8196]], requires_grad=True) tensor([[0.9692]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(1.8481) tensor(29.6694)\n",
      "t: 0.028147497671065627\n",
      "Epoch 4\n",
      "Objective + penalty: 12.55046558380127\n",
      "Vars: tensor([[0.7676]], requires_grad=True) tensor([[0.1341]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.1392) tensor(29.6694)\n",
      "t: 0.014411518807585602\n",
      "Epoch 5\n",
      "Objective + penalty: 10.15103816986084\n",
      "Vars: tensor([[0.3765]], requires_grad=True) tensor([[0.5617]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(22.5932) tensor(29.6694)\n",
      "t: 0.011529215046068483\n",
      "Epoch 6\n",
      "Objective + penalty: 8.771390914916992\n",
      "Vars: tensor([[0.6370]], requires_grad=True) tensor([[0.2196]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(5.8574) tensor(29.6694)\n",
      "t: 0.018014398509482003\n",
      "Epoch 7\n",
      "Objective + penalty: 7.736728668212891\n",
      "Vars: tensor([[0.7425]], requires_grad=True) tensor([[0.7541]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(2.9280) tensor(29.6694)\n",
      "t: 0.014411518807585602\n",
      "Epoch 8\n",
      "Objective + penalty: 5.265050888061523\n",
      "Vars: tensor([[0.7003]], requires_grad=True) tensor([[0.3265]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.7177) tensor(29.6694)\n",
      "t: 0.011529215046068483\n",
      "Epoch 9\n",
      "Objective + penalty: 5.234173774719238\n",
      "Vars: tensor([[0.7547]], requires_grad=True) tensor([[0.6686]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(2.7573) tensor(29.6694)\n",
      "t: 0.009223372036854787\n",
      "Epoch 10\n",
      "Objective + penalty: 3.7842869758605957\n",
      "Vars: tensor([[0.7292]], requires_grad=True) tensor([[0.3949]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.4488) tensor(29.6694)\n",
      "t: 0.0037778931862957215\n",
      "Epoch 11\n",
      "Objective + penalty: 2.369042158126831\n",
      "Vars: tensor([[0.6293]], requires_grad=True) tensor([[0.5070]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.1330) tensor(29.6694)\n",
      "t: 0.002417851639229262\n",
      "Epoch 12\n",
      "Objective + penalty: 2.0752930641174316\n",
      "Vars: tensor([[0.6925]], requires_grad=True) tensor([[0.4353]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.8576) tensor(29.6694)\n",
      "t: 0.0037778931862957215\n",
      "Epoch 13\n",
      "Objective + penalty: 1.5036189556121826\n",
      "Vars: tensor([[0.7109]], requires_grad=True) tensor([[0.5474]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.3707) tensor(29.6694)\n",
      "t: 0.0030223145490365774\n",
      "Epoch 14\n",
      "Objective + penalty: 1.3711178302764893\n",
      "Vars: tensor([[0.7007]], requires_grad=True) tensor([[0.4577]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.7107) tensor(29.6694)\n",
      "t: 0.002417851639229262\n",
      "Epoch 15\n",
      "Objective + penalty: 0.9755687117576599\n",
      "Vars: tensor([[0.7121]], requires_grad=True) tensor([[0.5294]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.3539) tensor(29.6694)\n",
      "t: 0.0019342813113834097\n",
      "Epoch 16\n",
      "Objective + penalty: 0.9225664138793945\n",
      "Vars: tensor([[0.7056]], requires_grad=True) tensor([[0.4720]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.6224) tensor(29.6694)\n",
      "t: 0.0015474250491067279\n",
      "Epoch 17\n",
      "Objective + penalty: 0.6372565031051636\n",
      "Vars: tensor([[0.7127]], requires_grad=True) tensor([[0.5179]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.3445) tensor(29.6694)\n",
      "t: 0.0009903520314283058\n",
      "Epoch 18\n",
      "Objective + penalty: 0.4850235879421234\n",
      "Vars: tensor([[0.7094]], requires_grad=True) tensor([[0.4886]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0919) tensor(29.6694)\n",
      "t: 0.00040564819207303417\n",
      "Epoch 19\n",
      "Objective + penalty: 0.3286277949810028\n",
      "Vars: tensor([[0.6988]], requires_grad=True) tensor([[0.5006]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.1061) tensor(29.6694)\n",
      "t: 0.0002596148429267419\n",
      "Epoch 20\n",
      "Objective + penalty: 0.30228346586227417\n",
      "Vars: tensor([[0.7059]], requires_grad=True) tensor([[0.4929]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.6171) tensor(29.6694)\n",
      "t: 0.00040564819207303417\n",
      "Epoch 21\n",
      "Objective + penalty: 0.23425498604774475\n",
      "Vars: tensor([[0.7077]], requires_grad=True) tensor([[0.5049]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.4143) tensor(29.6694)\n",
      "t: 0.00032451855365842736\n",
      "Epoch 22\n",
      "Objective + penalty: 0.22734040021896362\n",
      "Vars: tensor([[0.7066]], requires_grad=True) tensor([[0.4953]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.6034) tensor(29.6694)\n",
      "t: 0.0002596148429267419\n",
      "Epoch 23\n",
      "Objective + penalty: 0.17742066085338593\n",
      "Vars: tensor([[0.7078]], requires_grad=True) tensor([[0.5030]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.4131) tensor(29.6694)\n",
      "t: 0.00016615349947311482\n",
      "Epoch 24\n",
      "Objective + penalty: 0.1466250717639923\n",
      "Vars: tensor([[0.7073]], requires_grad=True) tensor([[0.4981]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0531) tensor(29.6694)\n",
      "t: 6.805647338418785e-05\n",
      "Epoch 25\n",
      "Objective + penalty: 0.13293392956256866\n",
      "Vars: tensor([[0.7055]], requires_grad=True) tensor([[0.5001]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.1993) tensor(29.6694)\n",
      "t: 5.444517870735028e-05\n",
      "Epoch 26\n",
      "Objective + penalty: 0.1315421164035797\n",
      "Vars: tensor([[0.7070]], requires_grad=True) tensor([[0.4985]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.5973) tensor(29.6694)\n",
      "t: 8.507059173023481e-05\n",
      "Epoch 27\n",
      "Objective + penalty: 0.11642804741859436\n",
      "Vars: tensor([[0.7074]], requires_grad=True) tensor([[0.5010]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.4197) tensor(29.6694)\n",
      "t: 6.805647338418785e-05\n",
      "Epoch 28\n",
      "Objective + penalty: 0.1161799281835556\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.4990]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0507) tensor(29.6694)\n",
      "t: 3.484491437270418e-05\n",
      "Epoch 29\n",
      "Objective + penalty: 0.11075765639543533\n",
      "Vars: tensor([[0.7062]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2095) tensor(29.6694)\n",
      "t: 2.7875931498163346e-05\n",
      "Epoch 30\n",
      "Objective + penalty: 0.11038868874311447\n",
      "Vars: tensor([[0.7070]], requires_grad=True) tensor([[0.4992]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.5972) tensor(29.6694)\n",
      "t: 5.444517870735028e-05\n",
      "Epoch 31\n",
      "Objective + penalty: 0.11012877523899078\n",
      "Vars: tensor([[0.7072]], requires_grad=True) tensor([[0.5008]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.4215) tensor(29.6694)\n",
      "t: 5.444517870735028e-05\n",
      "Epoch 32\n",
      "Objective + penalty: 0.11009452491998672\n",
      "Vars: tensor([[0.7070]], requires_grad=True) tensor([[0.4992]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.5960) tensor(29.6694)\n",
      "t: 4.3556142965880224e-05\n",
      "Epoch 33\n",
      "Objective + penalty: 0.10059225559234619\n",
      "Vars: tensor([[0.7072]], requires_grad=True) tensor([[0.5005]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.4213) tensor(29.6694)\n",
      "t: 2.7875931498163346e-05\n",
      "Epoch 34\n",
      "Objective + penalty: 0.09675335884094238\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.4997]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0511) tensor(29.6694)\n",
      "t: 1.1417981541647708e-05\n",
      "Epoch 35\n",
      "Objective + penalty: 0.09309656172990799\n",
      "Vars: tensor([[0.7068]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2184) tensor(13.6694)\n",
      "t: 1.4272476927059634e-05\n",
      "Epoch 36\n",
      "Objective + penalty: 0.09187860041856766\n",
      "Vars: tensor([[0.7072]], requires_grad=True) tensor([[0.5002]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(3.4214) tensor(29.6694)\n",
      "t: 9.134385233318167e-06\n",
      "Epoch 37\n",
      "Objective + penalty: 0.09051366150379181\n",
      "Vars: tensor([[0.7072]], requires_grad=True) tensor([[0.4999]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0521) tensor(29.6694)\n",
      "t: 5.846006549323628e-06\n",
      "Epoch 38\n",
      "Objective + penalty: 0.09024471044540405\n",
      "Vars: tensor([[0.7070]], requires_grad=True) tensor([[0.5001]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2212) tensor(29.6694)\n",
      "t: 4.676805239458903e-06\n",
      "Epoch 39\n",
      "Objective + penalty: 0.08883397281169891\n",
      "Vars: tensor([[0.7072]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0517) tensor(29.6694)\n",
      "t: 3.7414441915671226e-06\n",
      "Epoch 40\n",
      "Objective + penalty: 0.08860741555690765\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5001]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2216) tensor(29.6694)\n",
      "t: 2.9931553532536984e-06\n",
      "Epoch 41\n",
      "Objective + penalty: 0.0877593532204628\n",
      "Vars: tensor([[0.7072]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0514) tensor(29.6694)\n",
      "t: 2.394524282602959e-06\n",
      "Epoch 42\n",
      "Objective + penalty: 0.08756157755851746\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2219) tensor(29.6694)\n",
      "t: 1.9156194260823675e-06\n",
      "Epoch 43\n",
      "Objective + penalty: 0.08707143366336823\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0512) tensor(29.6694)\n",
      "t: 1.5324955408658941e-06\n",
      "Epoch 44\n",
      "Objective + penalty: 0.08689107745885849\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2221) tensor(29.6694)\n",
      "t: 1.2259964326927154e-06\n",
      "Epoch 45\n",
      "Objective + penalty: 0.08664853125810623\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0511) tensor(13.6694)\n",
      "t: 1.9156194260823675e-06\n",
      "Epoch 46\n",
      "Objective + penalty: 0.08662974089384079\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.5950) tensor(29.6694)\n",
      "t: 1.2259964326927154e-06\n",
      "Epoch 47\n",
      "Objective + penalty: 0.08640985190868378\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2219) tensor(29.6694)\n",
      "t: 6.277101735386704e-07\n",
      "Epoch 48\n",
      "Objective + penalty: 0.08617406338453293\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0506) tensor(29.6694)\n",
      "t: 4.017345110647491e-07\n",
      "Epoch 49\n",
      "Objective + penalty: 0.08602988719940186\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2220) tensor(29.6694)\n",
      "t: 2.5711008708143947e-07\n",
      "Epoch 50\n",
      "Objective + penalty: 0.08595667779445648\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0506) tensor(29.6694)\n",
      "t: 2.056880696651516e-07\n",
      "Epoch 51\n",
      "Objective + penalty: 0.08594030141830444\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2220) tensor(29.6694)\n",
      "t: 1.645504557321213e-07\n",
      "Epoch 52\n",
      "Objective + penalty: 0.08589760959148407\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0505) tensor(29.6694)\n",
      "t: 1.3164036458569703e-07\n",
      "Epoch 53\n",
      "Objective + penalty: 0.0858844742178917\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2221) tensor(29.6694)\n",
      "t: 1.0531229166855763e-07\n",
      "Epoch 54\n",
      "Objective + penalty: 0.08585836738348007\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0505) tensor(29.6694)\n",
      "t: 8.42498333348461e-08\n",
      "Epoch 55\n",
      "Objective + penalty: 0.08584731817245483\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2221) tensor(29.6694)\n",
      "t: 6.739986666787689e-08\n",
      "Epoch 56\n",
      "Objective + penalty: 0.08583544939756393\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0505) tensor(29.6694)\n",
      "t: 5.3919893334301516e-08\n",
      "Epoch 57\n",
      "Objective + penalty: 0.08582370728254318\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2221) tensor(29.6694)\n",
      "t: 4.313591466744121e-08\n",
      "Epoch 58\n",
      "Objective + penalty: 0.08582001179456711\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0505) tensor(29.6694)\n",
      "t: 3.450873173395297e-08\n",
      "Epoch 59\n",
      "Objective + penalty: 0.08580860495567322\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2221) tensor(29.6694)\n",
      "t: 2.2085588309729903e-08\n",
      "Epoch 60\n",
      "Objective + penalty: 0.08580470830202103\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(4.5947) tensor(29.6694)\n",
      "t: 3.450873173395297e-08\n",
      "Epoch 61\n",
      "Objective + penalty: 0.08579830825328827\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(11.8995) tensor(29.6694)\n",
      "t: 1.7668470647783922e-08\n",
      "Epoch 62\n",
      "Objective + penalty: 0.085797019302845\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(26.0505) tensor(29.6694)\n",
      "t: 1.1307821214581712e-08\n",
      "Epoch 63\n",
      "Objective + penalty: 0.08579440414905548\n",
      "Vars: tensor([[0.7071]], requires_grad=True) tensor([[0.5000]], requires_grad=True) tensor([10.8347])\n",
      "Grads: tensor(27.2221) tensor(29.6694)\n",
      "t: 9.046256971665371e-09\n",
      "Penalty tensor([[5.3644e-07]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7071]], requires_grad=True),\n",
       " tensor([[0.5000]], requires_grad=True))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sphere Manifold"
   ],
   "metadata": {
    "id": "zJrVf8TqfKzK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cpu')\n",
    "torch.manual_seed(0)\n",
    "n = 300\n",
    "# All the user-provided data (vector/matrix/tensor) must be in torch tensor format.\n",
    "# As PyTorch tensor is single precision by default, one must explicitly set `dtype=torch.double`.\n",
    "# Also, please make sure the device of provided torch tensor is the same as opts.torch_device.\n",
    "dtype = torch.double\n",
    "A = torch.randn((n,n)).to(device=device, dtype=dtype)\n",
    "A = .5*(A+A.T)"
   ],
   "metadata": {
    "id": "MVteRjGtD116",
    "ExecuteTime": {
     "end_time": "2025-03-27T18:18:45.432243Z",
     "start_time": "2025-03-27T18:18:45.404711Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "def f(x):\n",
    "    return -x.T @ A @ x\n",
    "\n",
    "def penalty(x):\n",
    "    return torch.abs(x.T @ x - 1)\n",
    "\n",
    "def phi1(x, mu):\n",
    "    return f(x) + mu * penalty(x)"
   ],
   "metadata": {
    "id": "fBlOQFtrfNJu",
    "ExecuteTime": {
     "end_time": "2025-03-27T18:18:45.449061Z",
     "start_time": "2025-03-27T18:18:45.439071Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "def backtracking_line_search(t_rho, c, x_eps, mu_rho, mu_eps):\n",
    "    x = torch.randn(n, 1, dtype=dtype, requires_grad=True)\n",
    "    with torch.no_grad():\n",
    "        x /= torch.norm(x)\n",
    "    mu = torch.tensor([100.], dtype=dtype)\n",
    "\n",
    "    for iteration in range(1000):\n",
    "        print(\"Iter\", iteration)\n",
    "\n",
    "        # Find an approximate minimizer xk of phi1(x; µk), starting at xk^s\n",
    "        for epoch in range(1000):\n",
    "            phi1_x_mu = phi1(x, mu)\n",
    "            phi1_x_mu.backward()\n",
    "\n",
    "            # if epoch % 100 == 99:\n",
    "            # print(\"Epoch\", epoch)\n",
    "            # print(\"Objective + penalty:\", phi1_x_mu.item())\n",
    "            # print(\"Vars:\", x, mu)\n",
    "            # print(\"Grads:\", torch.norm(x.grad))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if torch.norm(x.grad) ** 2 <= x_eps:\n",
    "                    break\n",
    "\n",
    "                t = 1\n",
    "                while t >= 1e-8 and phi1(x - t * x.grad, mu) - phi1_x_mu >= -c * t * torch.norm(x.grad) ** 2:\n",
    "                    t *= t_rho\n",
    "                print(\"t:\", t, torch.norm(x.grad))\n",
    "                # if t < 1e-8:\n",
    "                #     break\n",
    "\n",
    "                x -= t * x.grad\n",
    "                x.grad = None\n",
    "\n",
    "        h = penalty(x)\n",
    "        print(\"Objective\", phi1_x_mu)\n",
    "        print(\"Penalty\", h)\n",
    "        if h < 1e-5:  # if h(xk ) ≤ τ\n",
    "            break\n",
    "\n",
    "        # Choose new penalty parameter µk+1 > µk ;\n",
    "        if mu * h > mu_eps:\n",
    "            mu *= mu_rho\n",
    "\n",
    "        # Choose new starting point (stay as optimal x1, x2)\n",
    "\n",
    "        print()\n",
    "\n",
    "    return x"
   ],
   "metadata": {
    "id": "ak3FJwkFfdT5",
    "ExecuteTime": {
     "end_time": "2025-03-27T18:18:45.536033Z",
     "start_time": "2025-03-27T18:18:45.490928Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "x = backtracking_line_search(t_rho=0.8, c=0, x_eps=1e-5, mu_rho=1.1, mu_eps=1e-5)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9lfugKPhgKhd",
    "outputId": "c64992c0-1f4d-4736-d07e-d4ff9791e7a1",
    "ExecuteTime": {
     "end_time": "2025-03-27T18:18:56.046765Z",
     "start_time": "2025-03-27T18:18:45.537822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "t: 9.046256971665371e-09 tensor(200.7635, dtype=torch.float64)\n",
      "t: 1.7668470647783922e-08 tensor(202.1281, dtype=torch.float64)\n",
      "t: 1.7668470647783922e-08 tensor(200.7638, dtype=torch.float64)\n",
      "t: 1.7668470647783922e-08 tensor(202.1281, dtype=torch.float64)\n",
      "t: 1.7668470647783922e-08 tensor(200.7638, dtype=torch.float64)\n",
      "t: 1.7668470647783922e-08 tensor(202.1282, dtype=torch.float64)\n",
      "t: 1.7668470647783922e-08 tensor(200.7637, dtype=torch.float64)\n",
      "t: 1.4134776518227139e-08 tensor(202.1282, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7635, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1284, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7635, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1284, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7635, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1284, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7635, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1284, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7634, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1285, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7633, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1285, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7633, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1285, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7633, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1285, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7633, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1286, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7633, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1286, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7633, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1286, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7632, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1286, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7632, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1286, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7632, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1287, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7632, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1287, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7632, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1287, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7631, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1287, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7631, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1288, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7631, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1288, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7631, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1288, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7631, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1288, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7631, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1289, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7630, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1289, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7630, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1289, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7630, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1289, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7630, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1289, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7630, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1290, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7629, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1290, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7629, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1290, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7629, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1290, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7629, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1291, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7629, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1291, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7629, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1291, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7628, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1291, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7628, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1292, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7628, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1292, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7628, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1292, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7628, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1291, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7627, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1292, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7627, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1292, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7626, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1292, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7626, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1292, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7626, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1293, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7625, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1293, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7625, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1293, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7625, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1293, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7625, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1294, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7625, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1294, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7625, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1294, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7624, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1294, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7624, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1295, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7624, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1295, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7624, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1295, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7624, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1295, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7623, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1295, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7623, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1296, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7623, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1296, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7623, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1296, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7623, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1296, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7623, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1297, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7622, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1297, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7622, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1297, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7622, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1297, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7622, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1298, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7622, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1298, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7621, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1298, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7621, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1298, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7621, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1299, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7621, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1299, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7621, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1299, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7621, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1299, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7620, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1299, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7620, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1300, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7620, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1299, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7620, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1299, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7619, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1300, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7618, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1300, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7618, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1300, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7618, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1300, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7618, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1301, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7618, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1301, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7618, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1301, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7617, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1301, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7617, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1301, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7617, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1302, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7617, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1302, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7617, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1302, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7616, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1302, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7616, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1303, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7616, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1303, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7616, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1303, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7616, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1303, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7615, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1304, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7615, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1304, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7615, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1304, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7615, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1304, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7615, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1304, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7615, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1305, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7614, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1305, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7614, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1305, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7614, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1305, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7614, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1306, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7614, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1306, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7613, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1306, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7613, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1306, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7613, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1307, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7613, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1307, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7613, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1307, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7613, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1306, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7612, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1307, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7612, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1307, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7611, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1307, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7611, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1307, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7611, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1308, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7610, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1308, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7610, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1308, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7610, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1308, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7610, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1309, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7610, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1309, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7610, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1309, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7609, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1309, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7609, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1310, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7609, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1310, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7609, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1310, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7609, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1310, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7608, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1310, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7608, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1311, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7608, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1311, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7608, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1311, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7608, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1311, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7607, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1312, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7607, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1312, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7607, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1312, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7607, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1312, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7607, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1313, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7607, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1313, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7606, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1313, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7606, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1313, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7606, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1314, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7606, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1314, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7606, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1314, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7605, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1314, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7605, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1314, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7605, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1315, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7605, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1314, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7605, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1314, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7604, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1315, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7603, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1315, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7603, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1315, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7603, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1315, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7603, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1316, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7603, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1316, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7602, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1316, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7602, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1316, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7602, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1316, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7602, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1317, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7602, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1317, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7601, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1317, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7601, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1317, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7601, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1318, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7601, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1318, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7601, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1318, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7601, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1318, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7600, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1319, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7600, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1319, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7600, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1319, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7600, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1319, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7600, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1320, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7599, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1320, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7599, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1320, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7599, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1320, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7599, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1320, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7599, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1321, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7599, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1321, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7598, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1321, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7598, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1321, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7598, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1322, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7598, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1322, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7598, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1322, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7597, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1322, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7597, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1322, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7597, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1322, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7596, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1322, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7596, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1322, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7596, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1323, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7595, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1323, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7595, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1323, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7595, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1323, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7595, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1324, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7595, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1324, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7594, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1324, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7594, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1324, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7594, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1325, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7594, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1325, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7594, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1325, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7594, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1325, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7593, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1325, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7593, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1326, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7593, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1326, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7593, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1326, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7593, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1326, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7592, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1327, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7592, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1327, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7592, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1327, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7592, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1327, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7592, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1328, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7592, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1328, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7591, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1328, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7591, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1328, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7591, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1328, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7591, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1329, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7591, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1329, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7590, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1329, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7590, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1329, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7590, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1330, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7590, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1329, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7590, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1329, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7589, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1330, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7588, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1330, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7588, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1330, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7588, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1330, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7588, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1331, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7588, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1331, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7587, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1331, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7587, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1331, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7587, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1331, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7587, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1332, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7587, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1332, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7586, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1332, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7586, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1332, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7586, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1333, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7586, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1333, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7586, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1333, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7586, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1333, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7585, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1334, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7585, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1334, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7585, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1334, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7585, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1334, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7585, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1334, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7584, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1335, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7584, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1335, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7584, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1335, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7584, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1335, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7584, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1336, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7584, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1336, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7583, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1336, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7583, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1336, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7583, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1337, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7583, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1337, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7583, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1337, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7582, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1337, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7582, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1337, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7582, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1337, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7582, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1337, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7581, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1337, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7580, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1338, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7580, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1338, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7580, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1338, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7580, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1338, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7580, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1339, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7580, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1339, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7579, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1339, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7579, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1339, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7579, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1340, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7579, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1340, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7579, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1340, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7578, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1340, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7578, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1340, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7578, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1341, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7578, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1341, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7578, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1341, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7578, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1341, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7577, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1342, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7577, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1342, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7577, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1342, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7577, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1342, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7577, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1343, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7576, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1343, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7576, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1343, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7576, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1343, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7576, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1344, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7576, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1344, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7576, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1344, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7575, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1344, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7575, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1344, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7575, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1345, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7575, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1344, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7575, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1344, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7574, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1345, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7573, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1345, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7573, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1345, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7573, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1345, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7573, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1346, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7572, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1346, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7572, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1346, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7572, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1346, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7572, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1346, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7572, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1347, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7572, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1347, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7571, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1347, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7571, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1347, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7571, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1348, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7571, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1348, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7571, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1348, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7570, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1348, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7570, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1349, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7570, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1349, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7570, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1349, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7570, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1349, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7570, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1350, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7569, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1350, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7569, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1350, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7569, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1350, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7569, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1350, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7569, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1351, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7568, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1351, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7568, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1351, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7568, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1351, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7568, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1352, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7568, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1352, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7567, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1352, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7567, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1352, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7567, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1352, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7567, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1352, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7566, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1352, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7566, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1352, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7565, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1353, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7565, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1353, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7565, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1353, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7565, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1353, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7565, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1354, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7564, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1354, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7564, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1354, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7564, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1354, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7564, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1355, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7564, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1355, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7564, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1355, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7563, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1355, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7563, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1355, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7563, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1356, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7563, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1356, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7563, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1356, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7562, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1356, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7562, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1357, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7562, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1357, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7562, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1357, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7562, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1357, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7562, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1358, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7561, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1358, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7561, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1358, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7561, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1358, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7561, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1359, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7561, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1359, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7560, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1359, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7560, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1359, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7560, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1359, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7560, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1360, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7560, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1359, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7559, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1359, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7559, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1360, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7558, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1360, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7558, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1360, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7558, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1360, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7558, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1361, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7557, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1361, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7557, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1361, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7557, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1361, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7557, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1361, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7557, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1362, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7556, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1362, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7556, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1362, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7556, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1362, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7556, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1363, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7556, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1363, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7556, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1363, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7555, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1363, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7555, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1364, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7555, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1364, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7555, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1364, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7555, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1364, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7554, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1365, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7554, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1365, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7554, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1365, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7554, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1365, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7554, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1365, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7554, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1366, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7553, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1366, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7553, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1366, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7553, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1366, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7553, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1367, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7553, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1367, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7552, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1367, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7552, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1367, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7552, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1367, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7552, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1367, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7551, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1367, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7551, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1367, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7550, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1368, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7550, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1368, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7550, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1368, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7550, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1368, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7550, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1369, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7549, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1369, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7549, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1369, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7549, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1369, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7549, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1370, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7549, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1370, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7549, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1370, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7548, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1370, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7548, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1370, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7548, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1371, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7548, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1371, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7548, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1371, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7547, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1371, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7547, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1372, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7547, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1372, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7547, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1372, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7547, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1372, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7547, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1373, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7546, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1373, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7546, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1373, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7546, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1373, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7546, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1374, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7546, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1374, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7545, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1374, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7545, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1374, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7545, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1374, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7545, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1375, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7545, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1374, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7544, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1374, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7544, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1375, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7543, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1375, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7543, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1375, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7543, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1375, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7543, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1376, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7542, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1376, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7542, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1376, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7542, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1376, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7542, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1376, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7542, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1377, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7541, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1377, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7541, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1377, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7541, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1377, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7541, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1378, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7541, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1378, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7541, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1378, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7540, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1378, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7540, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1379, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7540, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1379, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7540, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1379, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7540, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1379, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7539, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1380, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7539, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1380, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7539, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1380, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7539, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1380, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7539, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1380, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7539, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1381, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7538, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1381, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7538, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1381, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7538, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1381, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7538, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1382, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7538, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1382, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7537, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1382, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7537, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1381, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7537, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(202.1382, dtype=torch.float64)\n",
      "t: 1.1307821214581712e-08 tensor(200.7537, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1382, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7536, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1382, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7535, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1382, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7535, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1383, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7535, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1383, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7535, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1383, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7535, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1383, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7535, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1384, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7534, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1384, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7534, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1384, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7534, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1384, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7534, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1385, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7534, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1385, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7533, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1385, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7533, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1385, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7533, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1386, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7533, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1386, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7533, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1386, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7533, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1386, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7532, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1386, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7532, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1387, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7532, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1387, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7532, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1387, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7532, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1387, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7531, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1388, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7531, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1388, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7531, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1388, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7531, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1388, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7531, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1389, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7531, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1389, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(200.7530, dtype=torch.float64)\n",
      "t: 9.046256971665371e-09 tensor(202.1389, dtype=torch.float64)\n",
      "Objective tensor([[-0.3489]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Penalty tensor([[2.1742e-06]], dtype=torch.float64, grad_fn=<AbsBackward0>)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "f(x)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PybrVXwMgLuP",
    "outputId": "e2fb4fda-a904-494c-8404-7ca8d649eaf7",
    "ExecuteTime": {
     "end_time": "2025-03-27T18:18:56.095987Z",
     "start_time": "2025-03-27T18:18:56.057462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3491]], dtype=torch.float64, grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sphere Manifold with Exact Penalty and PyGranSO"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:19:00.931270Z",
     "start_time": "2025-03-27T18:18:56.135775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from pygranso.pygranso import pygranso\n",
    "from pygranso.pygransoStruct import pygransoStruct"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:19:00.938220Z",
     "start_time": "2025-03-27T18:19:00.932875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def exact_penalty_with_pygranso(mu_rho, mu_eps):\n",
    "    x = torch.randn(n, 1, dtype=dtype)\n",
    "    # with torch.no_grad():\n",
    "    #     x /= torch.norm(x)\n",
    "    mu = torch.tensor([10.], dtype=dtype)\n",
    "\n",
    "    for iteration in range(1000):\n",
    "        print(\"Iter\", iteration)\n",
    "        \n",
    "        # PyGRANSO\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "        # variables and corresponding dimensions.\n",
    "        var_in = {\"x\": [n, 1]}\n",
    "        \n",
    "        def comb_fn(X_struct):\n",
    "            x = X_struct.x\n",
    "            \n",
    "            # objective function\n",
    "            phi1_x_mu = phi1(x, mu)\n",
    "        \n",
    "            # inequality constraint, matrix form\n",
    "            ci = None\n",
    "        \n",
    "            # equality constraint \n",
    "            ce = None\n",
    "        \n",
    "            return [phi1_x_mu,ci,ce]\n",
    "        \n",
    "        opts = pygransoStruct()\n",
    "        # option for switching QP solver. We only have osqp as the only qp solver in current version. Default is osqp\n",
    "        # opts.QPsolver = 'osqp'\n",
    "        \n",
    "        # set an intial point\n",
    "        # All the user-provided data (vector/matrix/tensor) must be in torch tensor format. \n",
    "        # As PyTorch tensor is single precision by default, one must explicitly set `dtype=torch.double`.\n",
    "        # Also, please make sure the device of provided torch tensor is the same as opts.torch_device.\n",
    "        opts.x0 = x\n",
    "        opts.torch_device = device\n",
    "        opts.print_level = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        soln = pygranso(var_spec = var_in,combined_fn = comb_fn, user_opts = opts)\n",
    "        end = time.time()\n",
    "        print(\"Total Wall Time: {}s\".format(end - start))\n",
    "        x = soln.final.x\n",
    "        \n",
    "        # Exact penalty update\n",
    "        \n",
    "        h = penalty(x)\n",
    "        print(\"Objective\", f(x))\n",
    "        print(\"Penalty\", h)\n",
    "        if h < 1e-5:  # if h(xk ) ≤ τ\n",
    "            break\n",
    "\n",
    "        # Choose new penalty parameter µk+1 > µk ;\n",
    "        if mu * h > mu_eps:\n",
    "            mu *= mu_rho\n",
    "\n",
    "        # Choose new starting point (stay as optimal x1, x2)\n",
    "\n",
    "        print()\n",
    "\n",
    "    return x"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:19:08.622565Z",
     "start_time": "2025-03-27T18:19:00.939379Z"
    }
   },
   "cell_type": "code",
   "source": "x = exact_penalty_with_pygranso(mu_rho=1.1, mu_eps=1e-5)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "Total Wall Time: 0.02765798568725586s\n",
      "Objective tensor([[-3854.7148]], dtype=torch.float64)\n",
      "Penalty tensor([[251.5613]], dtype=torch.float64)\n",
      "\n",
      "Iter 1\n",
      "Total Wall Time: 0.006993770599365234s\n",
      "Objective tensor([[-3854.7148]], dtype=torch.float64)\n",
      "Penalty tensor([[251.5613]], dtype=torch.float64)\n",
      "\n",
      "Iter 2\n",
      "Total Wall Time: 0.00447392463684082s\n",
      "Objective tensor([[-3854.7148]], dtype=torch.float64)\n",
      "Penalty tensor([[251.5613]], dtype=torch.float64)\n",
      "\n",
      "Iter 3\n",
      "Total Wall Time: 0.012392997741699219s\n",
      "Objective tensor([[-44344.1748]], dtype=torch.float64)\n",
      "Penalty tensor([[2970.1424]], dtype=torch.float64)\n",
      "\n",
      "Iter 4\n",
      "Total Wall Time: 0.04290604591369629s\n",
      "Objective tensor([[-239787.0472]], dtype=torch.float64)\n",
      "Penalty tensor([[11985.8505]], dtype=torch.float64)\n",
      "\n",
      "Iter 5\n",
      "Total Wall Time: 0.013826131820678711s\n",
      "Objective tensor([[-427883.4310]], dtype=torch.float64)\n",
      "Penalty tensor([[20964.0460]], dtype=torch.float64)\n",
      "\n",
      "Iter 6\n",
      "Total Wall Time: 0.028512954711914062s\n",
      "Objective tensor([[-877667.0923]], dtype=torch.float64)\n",
      "Penalty tensor([[40119.5802]], dtype=torch.float64)\n",
      "\n",
      "Iter 7\n",
      "Total Wall Time: 0.06715965270996094s\n",
      "Objective tensor([[-4616958.9159]], dtype=torch.float64)\n",
      "Penalty tensor([[228544.1839]], dtype=torch.float64)\n",
      "\n",
      "Iter 8\n",
      "Total Wall Time: 0.251600980758667s\n",
      "Objective tensor([[-9613665.3060]], dtype=torch.float64)\n",
      "Penalty tensor([[425730.7635]], dtype=torch.float64)\n",
      "\n",
      "Iter 9\n",
      "Total Wall Time: 0.2905762195587158s\n",
      "Objective tensor([[-11832187.6322]], dtype=torch.float64)\n",
      "Penalty tensor([[499689.0754]], dtype=torch.float64)\n",
      "\n",
      "Iter 10\n",
      "Total Wall Time: 6.913106918334961s\n",
      "Objective tensor([[-24.2859]], dtype=torch.float64)\n",
      "Penalty tensor([[4.4409e-16]], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "An interesting note: starting with `mu = 1` instead of `mu = 100` yields:\n",
    "```\n",
    "Iter 33\n",
    "Total Wall Time: 0.006292819976806641s\n",
    "Objective tensor([[-1.1599e+50]], dtype=torch.float64)\n",
    "Penalty tensor([[4.9290e+48]], dtype=torch.float64)\n",
    "\n",
    "Iter 34\n",
    "Total Wall Time: 0.9548580646514893s\n",
    "Objective tensor([[-24.2859]], dtype=torch.float64)\n",
    "Penalty tensor([[4.8850e-14]], dtype=torch.float64)\n",
    "```\n",
    "which results in large objectives before the mu value is sufficient."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It also seems that starting with large `mu` may lead to a slightly worse objective (e.g. `mu=100` vs `mu=10000`). This may be because PyGRANSO is better at finding global optima to `f` if the penalty term contributes too much. See https://www.youtube.com/watch?v=7Dvz2HbSyM8 and the textbook about ill conditioning."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Orthogonal RNN"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:19:11.146543Z",
     "start_time": "2025-03-27T18:19:08.624413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pygranso.pygranso import pygranso\n",
    "from pygranso.pygransoStruct import pygransoStruct\n",
    "from pygranso.private.getNvar import getNvarTorch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from pygranso.private.getObjGrad import getObjGradDL"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:22:57.868242Z",
     "start_time": "2025-03-27T18:22:49.848863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "torch.set_default_device(device)\n",
    "torch.set_default_dtype(dtype)\n",
    "\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 30\n",
    "num_layers = 1\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "double_precision = torch.double\n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.reshape(x,(batch_size,sequence_length,input_size))\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device=device, dtype=double_precision)\n",
    "        out, hidden = self.rnn(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device=device, dtype=double_precision)\n",
    "model.train()\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(datasets.MNIST(\n",
    "    root = './examples/data/mnist',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True,\n",
    "), [0.6, 0.4])\n",
    "\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(train_data,\n",
    "                                        batch_size=100,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=1),\n",
    "    'test' : torch.utils.data.DataLoader(test_data,\n",
    "                                        batch_size=100,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=1)\n",
    "}\n",
    "\n",
    "inputs, labels = next(iter(loaders['train']))\n",
    "inputs, labels = inputs.reshape(-1, sequence_length, input_size).to(device=device, dtype=double_precision), labels.to(device=device)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:23:18.005416Z",
     "start_time": "2025-03-27T18:23:17.990516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def f(model, inputs, labels):\n",
    "    logits = model(inputs)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    return loss_fn(logits, labels)\n",
    "\n",
    "def penalty(model):\n",
    "    A = list(model.parameters())[1]\n",
    "    \n",
    "    # print(A)\n",
    "    \n",
    "    return torch.norm(A.T @ A - torch.eye(hidden_size), p=1)\n",
    "\n",
    "def phi1(model, mu):\n",
    "    return f(model, inputs, labels) + mu * penalty(model)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:23:18.667481Z",
     "start_time": "2025-03-27T18:23:18.654977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def exact_penalty_with_pygranso(mu_rho, mu_eps):\n",
    "    mu = torch.tensor([1.], dtype=dtype)\n",
    "\n",
    "    for iteration in range(1000):\n",
    "        print(\"Iter\", iteration)\n",
    "        \n",
    "        # PyGRANSO\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "        def comb_fn(model):\n",
    "            # objective function\n",
    "            phi1_x_mu = phi1(model, mu)\n",
    "        \n",
    "            # inequality constraint, matrix form\n",
    "            ci = None\n",
    "        \n",
    "            # equality constraint \n",
    "            ce = None\n",
    "        \n",
    "            return [phi1_x_mu,ci,ce]\n",
    "        \n",
    "        opts = pygransoStruct()\n",
    "        # option for switching QP solver. We only have osqp as the only qp solver in current version. Default is osqp\n",
    "        # opts.QPsolver = 'osqp'\n",
    "        \n",
    "        # set an intial point\n",
    "        # All the user-provided data (vector/matrix/tensor) must be in torch tensor format. \n",
    "        # As PyTorch tensor is single precision by default, one must explicitly set `dtype=torch.double`.\n",
    "        # Also, please make sure the device of provided torch tensor is the same as opts.torch_device.\n",
    "        nvar = getNvarTorch(model.parameters())\n",
    "        opts.x0 = torch.nn.utils.parameters_to_vector(model.parameters()).detach().reshape(nvar,1)\n",
    "        opts.torch_device = device\n",
    "        opts.opt_tol = 1e-5\n",
    "        opts.viol_eq_tol = 1e-5\n",
    "        opts.print_level = 1\n",
    "        opts.print_frequency = 50\n",
    "        \n",
    "        # opts.maxit = 1000 yields 80% acc but seems far from reaching stationarity\n",
    "        #  900 ║  - │   -   ║  0.76603445858 ║   -  │   -  ║ QN │     6 │ 0.031250 ║     1 │ 0.246480   ║ \n",
    "        #  950 ║  - │   -   ║  0.70312720877 ║   -  │   -  ║ QN │     6 │ 0.031250 ║     1 │ 0.471985   ║ \n",
    "        # ═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
    "        #      ║ Penalty Fn ║                ║  Violation  ║ <--- Line Search ---> ║ <- Stationarity -> ║ \n",
    "        # Iter ║ Mu │ Value ║    Objective   ║ Ineq │  Eq  ║ SD │ Evals │     t    ║ Grads │    Value   ║ \n",
    "        # ═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
    "        # 1000 ║  - │   -   ║  0.62800096715 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.430504   ║ \n",
    "        opts.maxit = 3000\n",
    "        \n",
    "        start = time.time()\n",
    "        soln = pygranso(var_spec = model,combined_fn = comb_fn, user_opts = opts)\n",
    "        end = time.time()\n",
    "        print(\"Total Wall Time: {}s\".format(end - start))\n",
    "        torch.nn.utils.vector_to_parameters(soln.final.x, model.parameters())\n",
    "        \n",
    "        # Exact penalty update\n",
    "        \n",
    "        h = penalty(model)\n",
    "        print(\"Objective:\", f(model, inputs, labels))\n",
    "        print(\"Penalty:\", h)\n",
    "        if h < 1e-3:  # if h(xk ) ≤ τ\n",
    "            break\n",
    "\n",
    "        # Choose new penalty parameter µk+1 > µk ;\n",
    "        if mu * h > mu_eps:\n",
    "            mu *= mu_rho\n",
    "\n",
    "        # Choose new starting point (stay as optimal x1, x2)\n",
    "\n",
    "        print()\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:34:00.353290Z",
     "start_time": "2025-03-27T18:23:19.522300Z"
    }
   },
   "cell_type": "code",
   "source": "model = exact_penalty_with_pygranso(mu_rho=1.1, mu_eps=1e-5)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "\n",
      "\n",
      "\u001B[33m╔═════ QP SOLVER NOTICE ════════════════════════════════════════════════════════════════════════╗\n",
      "\u001B[0m\u001B[33m║  PyGRANSO requires a quadratic program (QP) solver that has a quadprog-compatible interface,  ║\n",
      "\u001B[0m\u001B[33m║  the default is osqp. Users may provide their own wrapper for the QP solver.                  ║\n",
      "\u001B[0m\u001B[33m║  To disable this notice, set opts.quadprog_info_msg = False                                   ║\n",
      "\u001B[0m\u001B[33m╚═══════════════════════════════════════════════════════════════════════════════════════════════╝\n",
      "\u001B[0m══════════════════════════════════════════════════════════════════════════════════════════════╗\n",
      "PyGRANSO: A PyTorch-enabled port of GRANSO with auto-differentiation                          ║ \n",
      "Version 1.2.0                                                                                 ║ \n",
      "Licensed under the AGPLv3, Copyright (C) 2021-2022 Tim Mitchell and Buyun Liang               ║ \n",
      "══════════════════════════════════════════════════════════════════════════════════════════════╣\n",
      "Problem specifications:                                                                       ║ \n",
      " # of variables                     :   2110                                                  ║ \n",
      " # of inequality constraints        :      0                                                  ║ \n",
      " # of equality constraints          :      0                                                  ║ \n",
      "═════╦════════════╦════════════════╦═════════════╦═══════════════════════╦════════════════════╣\n",
      "     ║ Penalty Fn ║                ║  Violation  ║ <--- Line Search ---> ║ <- Stationarity -> ║ \n",
      "Iter ║ Mu │ Value ║    Objective   ║ Ineq │  Eq  ║ SD │ Evals │     t    ║ Grads │    Value   ║ \n",
      "═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
      "   0 ║  - │   -   ║  65.5413232721 ║   -  │   -  ║ -  │     1 │ 0.000000 ║     1 │ 41.55839   ║ \n",
      "  50 ║  - │   -   ║  21.5933319046 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 2.591094   ║ \n",
      " 100 ║  - │   -   ║  6.58352786692 ║   -  │   -  ║ QN │     8 │ 0.007812 ║     1 │ 1.736525   ║ \n",
      " 150 ║  - │   -   ║  4.01381597149 ║   -  │   -  ║ QN │     9 │ 0.003906 ║     1 │ 2.355774   ║ \n",
      " 200 ║  - │   -   ║  3.20910192813 ║   -  │   -  ║ QN │     9 │ 0.003906 ║     1 │ 1.977312   ║ \n",
      " 250 ║  - │   -   ║  2.83975006252 ║   -  │   -  ║ QN │    10 │ 0.001953 ║     1 │ 1.314586   ║ \n",
      " 300 ║  - │   -   ║  2.64906891979 ║   -  │   -  ║ QN │    11 │ 9.77e-04 ║     1 │ 1.105413   ║ \n",
      " 350 ║  - │   -   ║  2.51225101837 ║   -  │   -  ║ QN │    10 │ 0.001953 ║     1 │ 1.182235   ║ \n",
      " 400 ║  - │   -   ║  2.41251105797 ║   -  │   -  ║ QN │    11 │ 9.77e-04 ║     1 │ 1.139229   ║ \n",
      " 450 ║  - │   -   ║  2.33995214217 ║   -  │   -  ║ QN │    12 │ 4.88e-04 ║     1 │ 0.536133   ║ \n",
      " 500 ║  - │   -   ║  2.29266759212 ║   -  │   -  ║ QN │    11 │ 9.77e-04 ║     1 │ 0.499285   ║ \n",
      " 550 ║  - │   -   ║  2.25935105955 ║   -  │   -  ║ QN │    10 │ 0.001953 ║     1 │ 0.186523   ║ \n",
      " 600 ║  - │   -   ║  2.21804949305 ║   -  │   -  ║ QN │     7 │ 0.015625 ║     1 │ 0.050703   ║ \n",
      " 650 ║  - │   -   ║  2.08186359898 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.132914   ║ \n",
      " 700 ║  - │   -   ║  1.59362128163 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.443838   ║ \n",
      " 750 ║  - │   -   ║  1.41278116892 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.111440   ║ \n",
      " 800 ║  - │   -   ║  1.21194803958 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.204434   ║ \n",
      " 850 ║  - │   -   ║  1.09033748137 ║   -  │   -  ║ QN │     6 │ 0.031250 ║     1 │ 0.204326   ║ \n",
      " 900 ║  - │   -   ║  1.01497770519 ║   -  │   -  ║ QN │     6 │ 0.031250 ║     1 │ 0.330951   ║ \n",
      " 950 ║  - │   -   ║  0.94243628221 ║   -  │   -  ║ QN │     6 │ 0.031250 ║     1 │ 1.101129   ║ \n",
      "═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
      "     ║ Penalty Fn ║                ║  Violation  ║ <--- Line Search ---> ║ <- Stationarity -> ║ \n",
      "Iter ║ Mu │ Value ║    Objective   ║ Ineq │  Eq  ║ SD │ Evals │     t    ║ Grads │    Value   ║ \n",
      "═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
      "1000 ║  - │   -   ║  0.86639967038 ║   -  │   -  ║ QN │     6 │ 0.031250 ║     1 │ 0.112740   ║ \n",
      "1050 ║  - │   -   ║  0.79970088635 ║   -  │   -  ║ QN │     6 │ 0.031250 ║     1 │ 0.084954   ║ \n",
      "1100 ║  - │   -   ║  0.71645033670 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.750786   ║ \n",
      "1150 ║  - │   -   ║  0.60933739595 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.223477   ║ \n",
      "1200 ║  - │   -   ║  0.52722506362 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.072473   ║ \n",
      "1250 ║  - │   -   ║  0.43272361151 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.196268   ║ \n",
      "1300 ║  - │   -   ║  0.38453110598 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.105591   ║ \n",
      "1350 ║  - │   -   ║  0.31916749232 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.191332   ║ \n",
      "1400 ║  - │   -   ║  0.25375643131 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.453763   ║ \n",
      "1450 ║  - │   -   ║  0.21378637207 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.675249   ║ \n",
      "1500 ║  - │   -   ║  0.16353418075 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.179632   ║ \n",
      "1550 ║  - │   -   ║  0.12357578979 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.653733   ║ \n",
      "1600 ║  - │   -   ║  0.10317329127 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.116985   ║ \n",
      "1650 ║  - │   -   ║  0.08396843661 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.333271   ║ \n",
      "1700 ║  - │   -   ║  0.07169954125 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.642395   ║ \n",
      "1750 ║  - │   -   ║  0.06420077783 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.264249   ║ \n",
      "1800 ║  - │   -   ║  0.05367598310 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.105823   ║ \n",
      "1850 ║  - │   -   ║  0.04466845521 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.330293   ║ \n",
      "1900 ║  - │   -   ║  0.03500404652 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.264133   ║ \n",
      "1950 ║  - │   -   ║  0.02878225335 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.054832   ║ \n",
      "═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
      "     ║ Penalty Fn ║                ║  Violation  ║ <--- Line Search ---> ║ <- Stationarity -> ║ \n",
      "Iter ║ Mu │ Value ║    Objective   ║ Ineq │  Eq  ║ SD │ Evals │     t    ║ Grads │    Value   ║ \n",
      "═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
      "2000 ║  - │   -   ║  0.02312037247 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.043250   ║ \n",
      "2050 ║  - │   -   ║  0.01962915758 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.141381   ║ \n",
      "2100 ║  - │   -   ║  0.01673394570 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.133822   ║ \n",
      "2150 ║  - │   -   ║  0.01444473770 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.126045   ║ \n",
      "2200 ║  - │   -   ║  0.01105255427 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.038679   ║ \n",
      "2250 ║  - │   -   ║  0.00976084894 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.313136   ║ \n",
      "2300 ║  - │   -   ║  0.00848315782 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.066087   ║ \n",
      "2350 ║  - │   -   ║  0.00729328115 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.085213   ║ \n",
      "2400 ║  - │   -   ║  0.00671151301 ║   -  │   -  ║ QN │     5 │ 0.062500 ║     1 │ 0.108465   ║ \n",
      "2450 ║  - │   -   ║  0.00626420473 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.053333   ║ \n",
      "2500 ║  - │   -   ║  0.00532963096 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.049705   ║ \n",
      "2550 ║  - │   -   ║  0.00488901775 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.077768   ║ \n",
      "2600 ║  - │   -   ║  0.00450485701 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.006994   ║ \n",
      "2650 ║  - │   -   ║  0.00401277681 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.021891   ║ \n",
      "2700 ║  - │   -   ║  0.00328491751 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.019170   ║ \n",
      "2750 ║  - │   -   ║  0.00305066084 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.015610   ║ \n",
      "2800 ║  - │   -   ║  0.00276432574 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.025571   ║ \n",
      "2850 ║  - │   -   ║  0.00248270275 ║   -  │   -  ║ QN │     4 │ 0.125000 ║     1 │ 0.019094   ║ \n",
      "2900 ║  - │   -   ║  0.00229498361 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.005182   ║ \n",
      "2950 ║  - │   -   ║  0.00213860985 ║   -  │   -  ║ QN │     3 │ 0.250000 ║     1 │ 0.048578   ║ \n",
      "═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
      "     ║ Penalty Fn ║                ║  Violation  ║ <--- Line Search ---> ║ <- Stationarity -> ║ \n",
      "Iter ║ Mu │ Value ║    Objective   ║ Ineq │  Eq  ║ SD │ Evals │     t    ║ Grads │    Value   ║ \n",
      "═════╬════════════╬════════════════╬═════════════╬═══════════════════════╬════════════════════╣\n",
      "3000 ║  - │   -   ║  0.00181329773 ║   -  │   -  ║ QN │     2 │ 0.500000 ║     1 │ 0.028938   ║ \n",
      "═════╩════════════╩════════════════╩═════════════╩═══════════════════════╩════════════════════╣\n",
      "Optimization results:                                                                         ║ \n",
      "F = final iterate, B = Best (to tolerance), MF = Most Feasible                                ║ \n",
      "═════╦════════════╦════════════════╦═════════════╦═══════════════════════╦════════════════════╣\n",
      "   F ║    │       ║  0.00181329773 ║   -  │   -  ║    │       │          ║       │            ║ \n",
      "   B ║    │       ║  0.00181329773 ║   -  │   -  ║    │       │          ║       │            ║ \n",
      "═════╩════════════╩════════════════╩═════════════╩═══════════════════════╩════════════════════╣\n",
      "Iterations:              3000                                                                 ║ \n",
      "Function evaluations:    15636                                                                ║ \n",
      "PyGRANSO termination code: 4 --- max iterations reached.                                      ║ \n",
      "══════════════════════════════════════════════════════════════════════════════════════════════╝\n",
      "Total Wall Time: 640.8021488189697s\n",
      "Objective: tensor(0.0018, grad_fn=<NllLossBackward0>)\n",
      "Penalty: tensor(4.0138e-05, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:34:00.416964Z",
     "start_time": "2025-03-27T18:34:00.362195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logits = model(inputs)\n",
    "_, predicted = torch.max(logits.data, 1)\n",
    "correct = (predicted == labels).sum().item()\n",
    "print(\"Final acc = {:.2f}%\".format((100 * correct/len(inputs))))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final acc = 100.00%\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:34:46.677855Z",
     "start_time": "2025-03-27T18:34:39.502428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "\n",
    "inputs, labels = next(iter(loaders['test']))\n",
    "inputs, labels = inputs.reshape(-1, sequence_length, input_size).to(device=device, dtype=double_precision), labels.to(device=device)\n",
    "\n",
    "logits = model(inputs)\n",
    "_, predicted = torch.max(logits.data, 1)\n",
    "correct = (predicted == labels).sum().item()\n",
    "print(\"Final acc = {:.2f}%\".format((100 * correct/len(inputs))))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final acc = 25.00%\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Orthogonal RNN with PyGRANSO"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:36:02.502349Z",
     "start_time": "2025-03-27T18:36:02.478352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device=device, dtype=double_precision)\n",
    "model.train()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): RNN(28, 30, batch_first=True)\n",
       "  (fc): Linear(in_features=30, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:36:15.252474Z",
     "start_time": "2025-03-27T18:36:15.229374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def user_fn(model,inputs,labels):\n",
    "    # objective function    \n",
    "    logits = model(inputs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    f = criterion(logits, labels)\n",
    "\n",
    "    A = list(model.parameters())[1]\n",
    "\n",
    "    # inequality constraint\n",
    "    ci = None\n",
    "\n",
    "    # equality constraint \n",
    "    # special orthogonal group\n",
    "    \n",
    "    ce = pygransoStruct()\n",
    "\n",
    "    c1_vec = (A.T @ A \n",
    "              - torch.eye(hidden_size)\n",
    "              .to(device=device, dtype=double_precision)\n",
    "             ).reshape(1,-1)\n",
    "    \n",
    "    ce.c1 = torch.linalg.vector_norm(c1_vec,2) # l2 folding to reduce the total number of constraints\n",
    "    # ce.c2 = torch.det(A) - 1\n",
    "\n",
    "    # ce = None\n",
    "\n",
    "    return [f,ci,ce]\n",
    "\n",
    "comb_fn = lambda model : user_fn(model,inputs,labels)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:36:20.645988Z",
     "start_time": "2025-03-27T18:36:20.636097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "opts = pygransoStruct()\n",
    "opts.torch_device = device\n",
    "nvar = getNvarTorch(model.parameters())\n",
    "opts.x0 = torch.nn.utils.parameters_to_vector(model.parameters()).detach().reshape(nvar,1)\n",
    "opts.opt_tol = 1e-3\n",
    "opts.viol_eq_tol = 1e-4\n",
    "# opts.maxit = 150\n",
    "# opts.fvalquit = 1e-6\n",
    "opts.print_level = 1\n",
    "opts.print_frequency = 50\n",
    "# opts.print_ascii = True\n",
    "# opts.limited_mem_size = 100\n",
    "opts.double_precision = True\n",
    "\n",
    "opts.mu0 = 1"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:36:25.783781Z",
     "start_time": "2025-03-27T18:36:25.751210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logits = model(inputs)\n",
    "_, predicted = torch.max(logits.data, 1)\n",
    "correct = (predicted == labels).sum().item()\n",
    "print(\"Initial acc = {:.2f}%\".format((100 * correct/len(inputs))))  "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial acc = 6.00%\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:41:35.219715Z",
     "start_time": "2025-03-27T18:36:31.010963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "soln = pygranso(var_spec= model, combined_fn = comb_fn, user_opts = opts)\n",
    "end = time.time()\n",
    "print(\"Total Wall Time: {}s\".format(end - start))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[33m╔═════ QP SOLVER NOTICE ════════════════════════════════════════════════════════════════════════╗\n",
      "\u001B[0m\u001B[33m║  PyGRANSO requires a quadratic program (QP) solver that has a quadprog-compatible interface,  ║\n",
      "\u001B[0m\u001B[33m║  the default is osqp. Users may provide their own wrapper for the QP solver.                  ║\n",
      "\u001B[0m\u001B[33m║  To disable this notice, set opts.quadprog_info_msg = False                                   ║\n",
      "\u001B[0m\u001B[33m╚═══════════════════════════════════════════════════════════════════════════════════════════════╝\n",
      "\u001B[0m═════════════════════════════════════════════════════════════════════════════════════════════════════════════════╗\n",
      "PyGRANSO: A PyTorch-enabled port of GRANSO with auto-differentiation                                             ║ \n",
      "Version 1.2.0                                                                                                    ║ \n",
      "Licensed under the AGPLv3, Copyright (C) 2021-2022 Tim Mitchell and Buyun Liang                                  ║ \n",
      "═════════════════════════════════════════════════════════════════════════════════════════════════════════════════╣\n",
      "Problem specifications:                                                                                          ║ \n",
      " # of variables                     :   2110                                                                     ║ \n",
      " # of inequality constraints        :      0                                                                     ║ \n",
      " # of equality constraints          :      1                                                                     ║ \n",
      "═════╦═══════════════════════════╦════════════════╦═════════════════╦═══════════════════════╦════════════════════╣\n",
      "     ║ <--- Penalty Function --> ║                ║ Total Violation ║ <--- Line Search ---> ║ <- Stationarity -> ║ \n",
      "Iter ║    Mu    │      Value     ║    Objective   ║ Ineq │    Eq    ║ SD │ Evals │     t    ║ Grads │    Value   ║ \n",
      "═════╬═══════════════════════════╬════════════════╬═════════════════╬═══════════════════════╬════════════════════╣\n",
      "   0 ║ 1.000000 │  6.43980293185 ║  2.34928175785 ║   -  │ 4.090521 ║ -  │     1 │ 0.000000 ║     1 │ 0.771164   ║ \n",
      "  50 ║ 1.000000 │  0.37239854699 ║  0.21709485389 ║   -  │ 0.155304 ║ S  │     3 │ 0.250000 ║     1 │ 0.786770   ║ \n",
      " 100 ║ 1.000000 │  0.02782197613 ║  0.00774549057 ║   -  │ 0.020076 ║ S  │     4 │ 0.125000 ║     1 │ 0.629255   ║ \n",
      " 150 ║ 1.000000 │  0.00930204433 ║  0.00357450159 ║   -  │ 0.005728 ║ S  │     4 │ 0.125000 ║     1 │ 0.329532   ║ \n",
      " 200 ║ 1.000000 │  0.00441251792 ║  0.00222308730 ║   -  │ 0.002189 ║ S  │     5 │ 0.062500 ║     1 │ 0.187941   ║ \n",
      " 250 ║ 1.000000 │  0.00250508840 ║  0.00166263267 ║   -  │ 8.42e-04 ║ S  │     9 │ 0.003906 ║     1 │ 0.293108   ║ \n",
      " 300 ║ 1.000000 │  0.00186806885 ║  0.00143974338 ║   -  │ 4.28e-04 ║ S  │     6 │ 0.031250 ║     1 │ 0.159249   ║ \n",
      " 350 ║ 1.000000 │  0.00152852683 ║  0.00124871082 ║   -  │ 2.80e-04 ║ S  │     9 │ 0.003906 ║     1 │ 0.040864   ║ \n",
      " 400 ║ 1.000000 │  0.00134877506 ║  0.00112370560 ║   -  │ 2.25e-04 ║ S  │    11 │ 9.77e-04 ║     1 │ 0.236013   ║ \n",
      " 450 ║ 1.000000 │  0.00121902635 ║  0.00108425657 ║   -  │ 1.35e-04 ║ S  │     8 │ 0.007812 ║     1 │ 0.297703   ║ \n",
      " 500 ║ 1.000000 │  0.00115435298 ║  0.00100191895 ║   -  │ 1.52e-04 ║ S  │    11 │ 9.77e-04 ║     2 │ 0.048333   ║ \n",
      " 550 ║ 1.000000 │  0.00104220263 ║  9.3175613e-04 ║   -  │ 1.10e-04 ║ S  │     6 │ 0.031250 ║     1 │ 0.110497   ║ \n",
      " 600 ║ 1.000000 │  9.8232742e-04 ║  8.6430449e-04 ║   -  │ 1.18e-04 ║ S  │     4 │ 0.125000 ║     1 │ 0.008946   ║ \n",
      " 650 ║ 1.000000 │  8.9638070e-04 ║  8.1007960e-04 ║   -  │ 8.63e-05 ║ S  │     3 │ 0.250000 ║     1 │ 0.146401   ║ \n",
      " 700 ║ 1.000000 │  8.2559862e-04 ║  7.5093147e-04 ║   -  │ 7.47e-05 ║ S  │     6 │ 0.031250 ║     1 │ 0.105594   ║ \n",
      " 750 ║ 1.000000 │  7.8582731e-04 ║  7.2922801e-04 ║   -  │ 5.66e-05 ║ S  │     8 │ 0.007812 ║     1 │ 0.140354   ║ \n",
      " 800 ║ 1.000000 │  7.5702178e-04 ║  7.2195843e-04 ║   -  │ 3.51e-05 ║ S  │     9 │ 0.003906 ║     1 │ 0.151441   ║ \n",
      " 850 ║ 1.000000 │  7.4210753e-04 ║  7.1759569e-04 ║   -  │ 2.45e-05 ║ S  │     8 │ 0.007812 ║     2 │ 0.007322   ║ \n",
      " 900 ║ 1.000000 │  7.3335997e-04 ║  7.1026192e-04 ║   -  │ 2.31e-05 ║ S  │     6 │ 0.031250 ║     2 │ 0.001451   ║ \n",
      " 950 ║ 1.000000 │  7.0925466e-04 ║  6.9097097e-04 ║   -  │ 1.83e-05 ║ S  │     5 │ 0.062500 ║     1 │ 0.028162   ║ \n",
      "═════╬═══════════════════════════╬════════════════╬═════════════════╬═══════════════════════╬════════════════════╣\n",
      "     ║ <--- Penalty Function --> ║                ║ Total Violation ║ <--- Line Search ---> ║ <- Stationarity -> ║ \n",
      "Iter ║    Mu    │      Value     ║    Objective   ║ Ineq │    Eq    ║ SD │ Evals │     t    ║ Grads │    Value   ║ \n",
      "═════╬═══════════════════════════╬════════════════╬═════════════════╬═══════════════════════╬════════════════════╣\n",
      "1000 ║ 1.000000 │  6.0237498e-04 ║  5.6076090e-04 ║   -  │ 4.16e-05 ║ S  │     3 │ 0.250000 ║     1 │ 0.034409   ║ \n",
      "═════╩═══════════════════════════╩════════════════╩═════════════════╩═══════════════════════╩════════════════════╣\n",
      "Optimization results:                                                                                            ║ \n",
      "F = final iterate, B = Best (to tolerance), MF = Most Feasible                                                   ║ \n",
      "═════╦═══════════════════════════╦════════════════╦═════════════════╦═══════════════════════╦════════════════════╣\n",
      "   F ║          │                ║  5.6076090e-04 ║   -  │ 4.16e-05 ║    │       │          ║       │            ║ \n",
      "   B ║          │                ║  5.5665691e-04 ║   -  │ 5.93e-05 ║    │       │          ║       │            ║ \n",
      "  MF ║          │                ║  7.0171966e-04 ║   -  │ 1.34e-05 ║    │       │          ║       │            ║ \n",
      "═════╩═══════════════════════════╩════════════════╩═════════════════╩═══════════════════════╩════════════════════╣\n",
      "Iterations:              1000                                                                                    ║ \n",
      "Function evaluations:    6571                                                                                    ║ \n",
      "PyGRANSO termination code: 4 --- max iterations reached.                                                         ║ \n",
      "═════════════════════════════════════════════════════════════════════════════════════════════════════════════════╝\n",
      "Total Wall Time: 304.1999628543854s\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:41:35.250603Z",
     "start_time": "2025-03-27T18:41:35.226417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.nn.utils.vector_to_parameters(soln.final.x, model.parameters())\n",
    "logits = model(inputs)\n",
    "_, predicted = torch.max(logits.data, 1)\n",
    "correct = (predicted == labels).sum().item()\n",
    "print(\"Final acc = {:.2f}%\".format((100 * correct/len(inputs))))  \n",
    "print(\"final feasibility = {}\".format(soln.final.tve))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final acc = 100.00%\n",
      "final feasibility = 4.1614077503424446e-05\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:41:39.885483Z",
     "start_time": "2025-03-27T18:41:35.254956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "\n",
    "inputs, labels = next(iter(loaders['test']))\n",
    "inputs, labels = inputs.reshape(-1, sequence_length, input_size).to(device=device, dtype=double_precision), labels.to(device=device)\n",
    "\n",
    "logits = model(inputs)\n",
    "_, predicted = torch.max(logits.data, 1)\n",
    "correct = (predicted == labels).sum().item()\n",
    "print(\"Final acc = {:.2f}%\".format((100 * correct/len(inputs))))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final acc = 43.00%\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
