{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5257bb27",
   "metadata": {},
   "source": [
    "# Perceptual Attack\n",
    "\n",
    "Apply neural perceptual attack to images taken from: Cassidy Laidlaw, Sahil Singla, and Soheil Feizi. [\"Perceptual adversarial robustness: Defense against unseen threat models.\"](https://arxiv.org/abs/2006.12655) arXiv preprint arXiv:2006.12655 (2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4364b1e6",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f7377",
   "metadata": {},
   "source": [
    "Given a classifier $f$ which maps any input image $x \\in X$ to its label $y = f(x) \\in Y$. The goal of neural perceptual attack is to find an input $\\widetilde{x}$ that is perceptually similar to the original input $x$ but can fool the classifier $f$. This can be formulated as:\n",
    "\n",
    "$$\\max_{\\widetilde{x}} L (f(\\widetilde{x}),y),$$\n",
    "$$\\text{s.t.}\\;\\; d(x,\\widetilde{x}) = ||\\phi(x) - \\phi (\\tilde{x}) ||_{2} \\leq \\epsilon$$\n",
    "Here $$L (f({x}),y) = \\max_{i\\neq y} (z_i(x) - z_y(x) ),$$\n",
    "where $z_i(x)$ is the $i$-th logit output of $f(x)$, and $\\phi(\\cdot)$ is a function that maps the input $x$ to  normalized, flattened activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dfdd50",
   "metadata": {},
   "source": [
    "## Modules Importing\n",
    "Import all necessary modules and add PyGRANSO src folder to system path. \n",
    "\n",
    "NOTE: the perceptual advex package (https://github.com/cassidylaidlaw/perceptual-advex.git) is required to calculate the distance "
   ]
  },
  {
   "cell_type": "code",
   "id": "23c19f28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T22:17:28.974568Z",
     "start_time": "2025-03-20T22:17:22.534785Z"
    }
   },
   "source": [
    "# install required package\n",
    "try:\n",
    "    import perceptual_advex\n",
    "except ImportError:\n",
    "    !pip install perceptual-advex"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting perceptual-advex\r\n",
      "  Downloading perceptual_advex-0.2.6-py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Requirement already satisfied: torch>=1.4.0 in /Users/jeffreyhu/Desktop/s25/csci-5527/pygranso/.venv/lib/python3.12/site-packages (from perceptual-advex) (2.6.0)\r\n",
      "Collecting robustness>=1.1.post2 (from perceptual-advex)\r\n",
      "  Downloading robustness-1.2.1.post2-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy>=1.18.2 in /Users/jeffreyhu/Desktop/s25/csci-5527/pygranso/.venv/lib/python3.12/site-packages (from perceptual-advex) (1.26.4)\r\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /Users/jeffreyhu/Desktop/s25/csci-5527/pygranso/.venv/lib/python3.12/site-packages (from perceptual-advex) (0.21.0)\r\n",
      "Collecting PyWavelets>=1.0.0 (from perceptual-advex)\r\n",
      "  Downloading pywavelets-1.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.0 kB)\r\n",
      "Collecting advex-uar>=0.0.5.dev0 (from perceptual-advex)\r\n",
      "  Downloading advex_uar-0.0.6.dev0-py3-none-any.whl.metadata (226 bytes)\r\n",
      "Collecting statsmodels==0.11.1 (from perceptual-advex)\r\n",
      "  Downloading statsmodels-0.11.1.tar.gz (15.4 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m15.4/15.4 MB\u001B[0m \u001B[31m25.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25lerror\r\n",
      "  \u001B[1;31merror\u001B[0m: \u001B[1msubprocess-exited-with-error\u001B[0m\r\n",
      "  \r\n",
      "  \u001B[31m×\u001B[0m \u001B[32mpip subprocess to install build dependencies\u001B[0m did not run successfully.\r\n",
      "  \u001B[31m│\u001B[0m exit code: \u001B[1;36m1\u001B[0m\r\n",
      "  \u001B[31m╰─>\u001B[0m \u001B[31m[60 lines of output]\u001B[0m\r\n",
      "  \u001B[31m   \u001B[0m Ignoring numpy: markers 'python_version == \"3.5\"' don't match your environment\r\n",
      "  \u001B[31m   \u001B[0m Ignoring numpy: markers 'python_version == \"3.6\"' don't match your environment\r\n",
      "  \u001B[31m   \u001B[0m Ignoring numpy: markers 'python_version == \"3.7\"' don't match your environment\r\n",
      "  \u001B[31m   \u001B[0m Collecting setuptools\r\n",
      "  \u001B[31m   \u001B[0m   Downloading setuptools-77.0.3-py3-none-any.whl.metadata (6.6 kB)\r\n",
      "  \u001B[31m   \u001B[0m Collecting wheel\r\n",
      "  \u001B[31m   \u001B[0m   Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "  \u001B[31m   \u001B[0m Collecting cython>=0.29.14\r\n",
      "  \u001B[31m   \u001B[0m   Downloading Cython-3.0.12-py2.py3-none-any.whl.metadata (3.3 kB)\r\n",
      "  \u001B[31m   \u001B[0m Collecting numpy==1.17.5\r\n",
      "  \u001B[31m   \u001B[0m   Downloading numpy-1.17.5.zip (6.4 MB)\r\n",
      "  \u001B[31m   \u001B[0m \u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/6.4 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\n",
      "  \u001B[31m   \u001B[0m \u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m \u001B[32m6.3/6.4 MB\u001B[0m \u001B[31m32.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "  \u001B[31m   \u001B[0m \u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.4/6.4 MB\u001B[0m \u001B[31m30.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "  \u001B[31m   \u001B[0m \u001B[?25h  Installing build dependencies: started\r\n",
      "  \u001B[31m   \u001B[0m   Installing build dependencies: finished with status 'done'\r\n",
      "  \u001B[31m   \u001B[0m   Getting requirements to build wheel: started\r\n",
      "  \u001B[31m   \u001B[0m   Getting requirements to build wheel: finished with status 'done'\r\n",
      "  \u001B[31m   \u001B[0m   Preparing metadata (pyproject.toml): started\r\n",
      "  \u001B[31m   \u001B[0m   Preparing metadata (pyproject.toml): finished with status 'error'\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[1;31merror\u001B[0m: \u001B[1msubprocess-exited-with-error\u001B[0m\r\n",
      "  \u001B[31m   \u001B[0m \r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m×\u001B[0m \u001B[32mPreparing metadata \u001B[0m\u001B[1;32m(\u001B[0m\u001B[32mpyproject.toml\u001B[0m\u001B[1;32m)\u001B[0m did not run successfully.\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m│\u001B[0m exit code: \u001B[1;36m1\u001B[0m\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m╰─>\u001B[0m \u001B[31m[25 lines of output]\u001B[0m\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m Running from numpy source directory.\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m <string>:419: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m Traceback (most recent call last):\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m   File \"/Users/jeffreyhu/Desktop/s25/csci-5527/pygranso/.venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m     main()\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m   File \"/Users/jeffreyhu/Desktop/s25/csci-5527/pygranso/.venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m   File \"/Users/jeffreyhu/Desktop/s25/csci-5527/pygranso/.venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 149, in prepare_metadata_for_build_wheel\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m     return hook(metadata_directory, config_settings)\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m   File \"/private/var/folders/b_/m0xhq6xs3gx3hc0kzw7d8sw40000gn/T/pip-build-env-69p0paa3/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 377, in prepare_metadata_for_build_wheel\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m     self.run_setup()\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m   File \"/private/var/folders/b_/m0xhq6xs3gx3hc0kzw7d8sw40000gn/T/pip-build-env-69p0paa3/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 522, in run_setup\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m     super().run_setup(setup_script=setup_script)\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m   File \"/private/var/folders/b_/m0xhq6xs3gx3hc0kzw7d8sw40000gn/T/pip-build-env-69p0paa3/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 320, in run_setup\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m     exec(code, locals())\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m   File \"<string>\", line 444, in <module>\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m   File \"<string>\", line 423, in setup_package\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m   File \"/private/var/folders/b_/m0xhq6xs3gx3hc0kzw7d8sw40000gn/T/pip-install-7357di_c/numpy_dc69a14c96f34213ad83caa090047f66/numpy/distutils/__init__.py\", line 6, in <module>\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m     from . import ccompiler\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m   File \"/private/var/folders/b_/m0xhq6xs3gx3hc0kzw7d8sw40000gn/T/pip-install-7357di_c/numpy_dc69a14c96f34213ad83caa090047f66/numpy/distutils/ccompiler.py\", line 111, in <module>\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m     replace_method(CCompiler, 'find_executables', CCompiler_find_executables)\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m                    ^^^^^^^^^\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m NameError: name 'CCompiler' is not defined. Did you mean: 'ccompiler'?\r\n",
      "  \u001B[31m   \u001B[0m   \u001B[31m   \u001B[0m \u001B[31m[end of output]\u001B[0m\r\n",
      "  \u001B[31m   \u001B[0m \r\n",
      "  \u001B[31m   \u001B[0m   \u001B[1;35mnote\u001B[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n",
      "  \u001B[31m   \u001B[0m \u001B[1;31merror\u001B[0m: \u001B[1mmetadata-generation-failed\u001B[0m\r\n",
      "  \u001B[31m   \u001B[0m \r\n",
      "  \u001B[31m   \u001B[0m \u001B[31m×\u001B[0m Encountered error while generating package metadata.\r\n",
      "  \u001B[31m   \u001B[0m \u001B[31m╰─>\u001B[0m See above for output.\r\n",
      "  \u001B[31m   \u001B[0m \r\n",
      "  \u001B[31m   \u001B[0m \u001B[1;35mnote\u001B[0m: This is an issue with the package mentioned above, not pip.\r\n",
      "  \u001B[31m   \u001B[0m \u001B[1;36mhint\u001B[0m: See above for details.\r\n",
      "  \u001B[31m   \u001B[0m \u001B[31m[end of output]\u001B[0m\r\n",
      "  \r\n",
      "  \u001B[1;35mnote\u001B[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\u001B[1;31merror\u001B[0m: \u001B[1msubprocess-exited-with-error\u001B[0m\r\n",
      "\r\n",
      "\u001B[31m×\u001B[0m \u001B[32mpip subprocess to install build dependencies\u001B[0m did not run successfully.\r\n",
      "\u001B[31m│\u001B[0m exit code: \u001B[1;36m1\u001B[0m\r\n",
      "\u001B[31m╰─>\u001B[0m See above for output.\r\n",
      "\r\n",
      "\u001B[1;35mnote\u001B[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n",
      "\u001B[?25h"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "90ed32f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T22:17:30.784167Z",
     "start_time": "2025-03-20T22:17:28.975921Z"
    }
   },
   "source": [
    "import time\n",
    "import torch\n",
    "import sys\n",
    "from pygranso.pygranso import pygranso\n",
    "from pygranso.pygransoStruct import pygransoStruct\n",
    "from pygranso.private.getNvar import getNvarTorch\n",
    "from perceptual_advex.utilities import get_dataset_model\n",
    "from perceptual_advex.perceptual_attacks import get_lpips_model\n",
    "from perceptual_advex.distances import normalize_flatten_features\n",
    "import gc"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'perceptual_advex'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpygranso\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpygransoStruct\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m pygransoStruct\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpygranso\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprivate\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgetNvar\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m getNvarTorch\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mperceptual_advex\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutilities\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m get_dataset_model\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mperceptual_advex\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mperceptual_attacks\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m get_lpips_model\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mperceptual_advex\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistances\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m normalize_flatten_features\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'perceptual_advex'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "4d0c82d9",
   "metadata": {},
   "source": [
    "## Download Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "beedb704",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# Download ResNet model\n",
    "if not os.path.exists('data/checkpoints/cifar_pgd_l2_1.pt'):\n",
    "    !mkdir -p data/checkpoints\n",
    "    !curl -o data/checkpoints/cifar_pgd_l2_1.pt https://perceptual-advex.s3.us-east-2.amazonaws.com/cifar_pgd_l2_1_cpu.pt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17a1b7fe",
   "metadata": {},
   "source": [
    "## Data Initialization \n",
    "\n",
    "Specify torch device, neural network architecture, and generate data.\n",
    "\n",
    "NOTE: please specify path for downloading data.\n",
    "\n",
    "Use GPU for this problem. If no cuda device available, please set *device = torch.device('cpu')*"
   ]
  },
  {
   "cell_type": "code",
   "id": "8b4842e1",
   "metadata": {},
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "dataset, model = get_dataset_model(\n",
    "dataset='cifar',\n",
    "arch='resnet50',\n",
    "checkpoint_fname='data/checkpoints/cifar_pgd_l2_1.pt',\n",
    ")\n",
    "model = model.to(device=device, dtype=torch.double)\n",
    "# Create a validation set loader.\n",
    "batch_size = 1\n",
    "_, val_loader = dataset.make_loaders(1, batch_size, only_val=True, shuffle_val=False)\n",
    "\n",
    "inputs, labels = next(iter(val_loader))\n",
    "\n",
    "# All the user-provided data (vector/matrix/tensor) must be in torch tensor format. \n",
    "# As PyTorch tensor is single precision by default, one must explicitly set `dtype=torch.double`.\n",
    "# Also, please make sure the device of provided torch tensor is the same as opts.torch_device.\n",
    "inputs = inputs.to(device=device, dtype=torch.double)\n",
    "labels = labels.to(device=device)\n",
    "\n",
    "# externally-bounded attack: AlexNet for constraint while ResNet for objective\n",
    "lpips_model = get_lpips_model('alexnet_cifar', model).to(device=device, dtype=torch.double)\n",
    "\n",
    "# Don't reccoment use in the current version. self-bounded attack: AlexNet for both constraint and objective\n",
    "# model = get_lpips_model('alexnet_cifar', model).to(device=device, dtype=torch.double)\n",
    "\n",
    "# attack_type = 'L_2'\n",
    "# attack_type = 'L_inf'\n",
    "attack_type = 'Perceptual'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ec80716b",
   "metadata": {},
   "source": [
    "## Function Set-Up\n",
    "\n",
    "Encode the optimization variables, and objective and constraint functions.\n",
    "\n",
    "Note: please strictly follow the format of comb_fn, which will be used in the PyGRANSO main algortihm."
   ]
  },
  {
   "cell_type": "code",
   "id": "fb360e75",
   "metadata": {},
   "source": [
    "# variables and corresponding dimensions.\n",
    "var_in = {\"x_tilde\": list(inputs.shape)}\n",
    "\n",
    "def MarginLoss(logits,labels):\n",
    "    correct_logits = torch.gather(logits, 1, labels.view(-1, 1))\n",
    "    max_2_logits, argmax_2_logits = torch.topk(logits, 2, dim=1)\n",
    "    top_max, second_max = max_2_logits.chunk(2, dim=1)\n",
    "    top_argmax, _ = argmax_2_logits.chunk(2, dim=1)\n",
    "    labels_eq_max = top_argmax.squeeze().eq(labels).float().view(-1, 1)\n",
    "    labels_ne_max = top_argmax.squeeze().ne(labels).float().view(-1, 1)\n",
    "    max_incorrect_logits = labels_eq_max * second_max + labels_ne_max * top_max\n",
    "    loss = -(max_incorrect_logits - correct_logits).clamp(max=1).squeeze().sum()\n",
    "    return loss\n",
    "\n",
    "def user_fn(X_struct,inputs,labels,lpips_model,model):\n",
    "    adv_inputs = X_struct.x_tilde\n",
    "    \n",
    "    # objective function\n",
    "    # 8/255 for L_inf, 1 for L_2, 0.5 for PPGD/LPA\n",
    "    if attack_type == 'L_2':\n",
    "        epsilon = 1\n",
    "    elif attack_type == 'L_inf':\n",
    "        epsilon = 8/255\n",
    "    else:\n",
    "        epsilon = 0.5\n",
    "\n",
    "    logits_outputs = model(adv_inputs)\n",
    "\n",
    "    f = MarginLoss(logits_outputs,labels)\n",
    "\n",
    "    # inequality constraint\n",
    "    ci = pygransoStruct()\n",
    "    if attack_type == 'L_2':\n",
    "        ci.c1 = torch.norm((inputs - adv_inputs).reshape(inputs.size()[0], -1)) - epsilon\n",
    "    elif attack_type == 'L_inf':\n",
    "        # ci.c1 = torch.norm((inputs - adv_inputs).reshape(inputs.size()[0], -1), float('inf')) - epsilon\n",
    "        linf_distance = torch.amax(torch.abs(inputs-adv_inputs).reshape(inputs.size()[0], -1))\n",
    "        ci.c1 = linf_distance - epsilon\n",
    "    else:\n",
    "        input_features = normalize_flatten_features( lpips_model.features(inputs)).detach()\n",
    "        adv_features = lpips_model.features(adv_inputs)\n",
    "        adv_features = normalize_flatten_features(adv_features)\n",
    "        lpips_dists = (adv_features - input_features).norm(dim=1)\n",
    "        ci.c1 = lpips_dists - epsilon\n",
    "    \n",
    "    # equality constraint \n",
    "    ce = None\n",
    "\n",
    "    return [f,ci,ce]\n",
    "\n",
    "comb_fn = lambda X_struct : user_fn(X_struct,inputs,labels,lpips_model,model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f0f55ace",
   "metadata": {},
   "source": [
    "## User Options\n",
    "Specify user-defined options for PyGRANSO "
   ]
  },
  {
   "cell_type": "code",
   "id": "f3a65b57",
   "metadata": {},
   "source": [
    "opts = pygransoStruct()\n",
    "opts.torch_device = device\n",
    "opts.maxit = 100\n",
    "opts.opt_tol = 1e-6\n",
    "opts.print_frequency = 1\n",
    "opts.x0 = torch.reshape(inputs,(torch.numel(inputs),1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8bca18c7",
   "metadata": {},
   "source": [
    "## Main Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "id": "632976b3",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "soln = pygranso(var_spec = var_in,combined_fn = comb_fn,user_opts = opts)\n",
    "end = time.time()\n",
    "print(\"Total Wall Time: {}s\".format(end - start))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3dc1ca84",
   "metadata": {},
   "source": [
    "## Batch Attacks\n",
    "\n",
    "Apply attacks to multiple images by repeating above steps and calculate the success rate"
   ]
  },
  {
   "cell_type": "code",
   "id": "49584c22",
   "metadata": {},
   "source": [
    "total_count = 50\n",
    "total_diff = 0\n",
    "original_count = 0\n",
    "attack_count = 0\n",
    "total_time = 0\n",
    "total_iterations = 0  \n",
    "i = 0\n",
    "it = iter(val_loader)\n",
    "\n",
    "for i in range(total_count):\n",
    "    # Get a batch from the validation set.\n",
    "    inputs, labels = next(it)\n",
    "    inputs = inputs.to(device=device, dtype=torch.double)\n",
    "    labels = labels.to(device=device)\n",
    "\n",
    "    # variables and corresponding dimensions.\n",
    "    var_in = {\"x_tilde\": list(inputs.shape)}\n",
    "\n",
    "    opts.x0 = torch.reshape(inputs,(torch.numel(inputs),1))\n",
    "    # suppress output\n",
    "    opts.print_level = 0\n",
    "\n",
    "    pred_labels = model(inputs).argmax(1)\n",
    "    if pred_labels == labels:\n",
    "        original_count += 1\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    start = time.time()\n",
    "    soln = pygranso(var_spec = var_in,combined_fn = comb_fn,user_opts = opts)\n",
    "    end = time.time()\n",
    "    \n",
    "    # Garbage Collector\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"attack image # %d\"%i)\n",
    "    \n",
    "    total_time += end - start\n",
    "    total_iterations += soln.fn_evals\n",
    "\n",
    "    final_adv_input = torch.reshape(soln.final.x,inputs.shape)\n",
    "    pred_labels2 = model(final_adv_input.to(device=device, dtype=torch.double)).argmax(1)\n",
    "\n",
    "    if pred_labels2 == labels:\n",
    "        attack_count += 1\n",
    "        \n",
    "    if attack_type == 'L_2':\n",
    "            diff = torch.norm((inputs.to(device=device, dtype=torch.double) - final_adv_input).reshape(inputs.size()[0], -1))\n",
    "    elif attack_type == 'L_inf':\n",
    "        diff = ( torch.norm((inputs.to(device=device, dtype=torch.double) - final_adv_input).reshape(inputs.size()[0], -1), float('inf') ) )\n",
    "    else:\n",
    "        input_features = normalize_flatten_features( lpips_model.features(inputs)).detach()\n",
    "        adv_features = lpips_model.features(final_adv_input)\n",
    "        adv_features = normalize_flatten_features(adv_features)\n",
    "        lpips_dists = torch.mean((adv_features - input_features).norm(dim=1))\n",
    "        diff = lpips_dists\n",
    "\n",
    "    total_diff += diff\n",
    "\n",
    "print(\"\\n\\n\\nModel train acc on the original image = {}\".format(( original_count/total_count )))\n",
    "print(\"Success rate of attack = {}\".format( (original_count-attack_count)/original_count ))\n",
    "print(\"Average distance between attacked image and original image = {}\".format(total_diff/original_count))\n",
    "print(\"Average run time of PyGRANSO = {}s, mean f_eval = {} iters\".format(total_time/original_count,total_iterations/original_count))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "04f2eee2",
   "metadata": {},
   "source": [
    "## ImageNet Datasets\n",
    "\n",
    "*(Optional)* Perceptual Attack on ImageNet datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3132a4d",
   "metadata": {},
   "source": [
    "### Modules Importing\n",
    "\n",
    "Import all necessary modules and add PyGRANSO src folder to system path.\n",
    "\n",
    "NOTE: the perceptual advex package (https://github.com/cassidylaidlaw/perceptual-advex.git) is required to calculate the distance"
   ]
  },
  {
   "cell_type": "code",
   "id": "375a30d8",
   "metadata": {},
   "source": [
    "import time\n",
    "import torch\n",
    "import sys\n",
    "## Adding PyGRANSO directories. Should be modified by user\n",
    "sys.path.append('/home/buyun/Documents/GitHub/PyGRANSO')\n",
    "from pygranso.pygranso import pygranso\n",
    "from pygranso.pygransoStruct import pygransoStruct\n",
    "from perceptual_advex.distances import normalize_flatten_features\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "import os\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7dbb0e2c",
   "metadata": {},
   "source": [
    "### Model Initialization\n",
    "Specify torch device, neural network architecture.\n",
    "\n",
    "NOTE: please specify path for downloading data.\n",
    "\n",
    "Use GPU for this problem. If no cuda device available, please set device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "id": "72da75de",
   "metadata": {},
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "class ResNet_orig_LPIPS(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=False):\n",
    "        super().__init__()\n",
    "        pretrained = bool(pretrained)\n",
    "        print(\"Use pytorch pretrained weights: [{}]\".format(pretrained))\n",
    "        self.back = resnet50(pretrained=pretrained)\n",
    "        self.back.fc = nn.Linear(2048, \n",
    "                                 num_classes)\n",
    "        # ===== Truncate the back and append the model to enable attack models\n",
    "        model_list = list(self.back.children())\n",
    "        self.head = nn.Sequential(\n",
    "            *model_list[0:4]\n",
    "        )\n",
    "        self.layer1 = model_list[4]\n",
    "        self.layer2 = model_list[5]\n",
    "        self.layer3 = model_list[6]\n",
    "        self.layer4 = model_list[7]\n",
    "        self.tail = nn.Sequential(\n",
    "            *[model_list[8],\n",
    "              nn.Flatten(),\n",
    "              model_list[9]]\n",
    "            )    \n",
    "        # print()    \n",
    "\n",
    "    def features(self, x):\n",
    "        \"\"\"\n",
    "            This function is called to produce perceptual features.\n",
    "            Output ==> has to be a tuple of conv features.\n",
    "        \"\"\"\n",
    "        x = x.type(self.back.fc.weight.dtype)\n",
    "        x = self.head(x)\n",
    "        x_layer1 = self.layer1(x)\n",
    "        x_layer2 = self.layer2(x_layer1)\n",
    "        x_layer3 = self.layer3(x_layer2)\n",
    "        x_layer4 = self.layer4(x_layer3)\n",
    "        return x_layer1, x_layer2, x_layer3, x_layer4\n",
    "    \n",
    "    def classifier(self, last_layer):\n",
    "        last_layer = self.tail(last_layer)\n",
    "        return last_layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x)[-1])\n",
    "    \n",
    "    def features_logits(self, x):\n",
    "        features = self.features(x)\n",
    "        logits = self.classifier(features[-1])\n",
    "        return features, logits\n",
    "\n",
    "base_model = ResNet_orig_LPIPS(num_classes=100,pretrained=False).to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4bd2e18",
   "metadata": {},
   "source": [
    "### Download Pretrained Model\n",
    "please download our pretrained model from the Google Drive [https://drive.google.com/file/d/1TZoKfVrqLgwKLa5-Y69uVOYb1xtOp8ty/view?usp=sharing] and add it to the \"data/checkpoints/\" path"
   ]
  },
  {
   "cell_type": "code",
   "id": "b4b4c7e3",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# Download Pretrained model\n",
    "if not os.path.exists('data/checkpoints/checkpoint.pth'):\n",
    "    !mkdir -p data/checkpoints\n",
    "        \n",
    "pretrained_path = os.path.join(\"data/checkpoints/\",\"checkpoint.pth\")\n",
    "state_dict = torch.load(pretrained_path)[\"model_state_dict\"]\n",
    "base_model.load_state_dict(state_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "23c8c70b",
   "metadata": {},
   "source": [
    "### Data Initialization\n",
    "Download ImageNet 2012 validation data from [https://www.image-net.org/download.php], and put it under the directory"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a17c9a1",
   "metadata": {},
   "source": [
    "# The ImageNet dataset is no longer publicly accessible. \n",
    "# You need to download the archives externally and place them in the root directory\n",
    "valset = datasets.ImageNet('/home/buyun/Documents/datasets/ImageNet/', split='val', transform=transforms.Compose([transforms.CenterCrop(224),transforms.ToTensor()]))\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=1,shuffle=False, num_workers=0, collate_fn=None, pin_memory=False,)\n",
    "\n",
    "# inputs, labels = next(iter(val_loader))\n",
    "\n",
    "i=0\n",
    "for inputs, labels in val_loader:\n",
    "    i+=1\n",
    "    if i > 2:\n",
    "        break\n",
    "\n",
    "# All the user-provided data (vector/matrix/tensor) must be in torch tensor format.\n",
    "# As PyTorch tensor is single precision by default, one must explicitly set `dtype=torch.double`.\n",
    "# Also, please make sure the device of provided torch tensor is the same as opts.torch_device.\n",
    "inputs = inputs.to(device=device, dtype=torch.double)\n",
    "labels = labels.to(device=device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "336a71da",
   "metadata": {},
   "source": [
    "### Function Set-Up\n",
    "Encode the optimization variables, and objective and constraint functions.\n",
    "\n",
    "Note: please strictly follow the format of comb_fn, which will be used in the PyGRANSO main algortihm."
   ]
  },
  {
   "cell_type": "code",
   "id": "90d5ed77",
   "metadata": {},
   "source": [
    "# variables and corresponding dimensions.\n",
    "var_in = {\"x_tilde\": list(inputs.shape)}\n",
    "\n",
    "def MarginLoss(logits,labels):\n",
    "    correct_logits = torch.gather(logits, 1, labels.view(-1, 1))\n",
    "    max_2_logits, argmax_2_logits = torch.topk(logits, 2, dim=1)\n",
    "    top_max, second_max = max_2_logits.chunk(2, dim=1)\n",
    "    top_argmax, _ = argmax_2_logits.chunk(2, dim=1)\n",
    "    labels_eq_max = top_argmax.squeeze().eq(labels).float().view(-1, 1)\n",
    "    labels_ne_max = top_argmax.squeeze().ne(labels).float().view(-1, 1)\n",
    "    max_incorrect_logits = labels_eq_max * second_max + labels_ne_max * top_max\n",
    "    loss = -(max_incorrect_logits - correct_logits).clamp(max=1).squeeze().sum()\n",
    "    return loss\n",
    "\n",
    "def user_fn(X_struct, inputs, labels, lpips_model, model, attack_type, eps=0.5):\n",
    "    adv_inputs = X_struct.x_tilde\n",
    "    epsilon = eps\n",
    "    logits_outputs = model(adv_inputs)\n",
    "    f = -torch.nn.functional.cross_entropy(logits_outputs,labels)\n",
    "\n",
    "    # inequality constraint\n",
    "    ci = pygransoStruct()\n",
    "    if attack_type == 'L_2':\n",
    "        ci.c1 = torch.norm((inputs - adv_inputs).reshape(inputs.size()[0], -1)) - epsilon\n",
    "    elif attack_type == 'L_inf':\n",
    "        ci.c1 = torch.norm((inputs - adv_inputs).reshape(inputs.size()[0], -1), float('inf')) - epsilon\n",
    "    else:\n",
    "        input_features = normalize_flatten_features( lpips_model.features(inputs)).detach()\n",
    "        adv_features = lpips_model.features(adv_inputs)\n",
    "        adv_features = normalize_flatten_features(adv_features)\n",
    "        lpips_dists = (adv_features - input_features).norm(dim=1)\n",
    "        ci.c1 = lpips_dists - epsilon\n",
    "\n",
    "    # equality constraint\n",
    "    ce = None\n",
    "    return [f,ci,ce]\n",
    "\n",
    "attack_type = \"Perceptual\"\n",
    "var_in = {\"x_tilde\": list(inputs.shape)}\n",
    "\n",
    "comb_fn = lambda X_struct : user_fn(X_struct, inputs, labels, lpips_model=base_model, model=base_model, attack_type=attack_type, eps=0.25)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bd23824e",
   "metadata": {},
   "source": [
    "### User Options\n",
    "Specify user-defined options for PyGRANSO"
   ]
  },
  {
   "cell_type": "code",
   "id": "aa6ecb07",
   "metadata": {},
   "source": [
    "opts = pygransoStruct()\n",
    "opts.torch_device = device\n",
    "opts.maxit = 50\n",
    "opts.opt_tol = 1e-4\n",
    "opts.viol_ineq_tol = 1e-4\n",
    "\n",
    "opts.print_frequency = 1\n",
    "opts.limited_mem_size = 100\n",
    "opts.x0 = torch.reshape(inputs,(torch.numel(inputs),1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cffa6874",
   "metadata": {},
   "source": [
    "### Main Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "id": "7c01cb23",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "soln = pygranso(var_spec = var_in,combined_fn = comb_fn,user_opts = opts)\n",
    "end = time.time()\n",
    "print(\"Total Wall Time: {}s\".format(end - start))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9ba71567",
   "metadata": {},
   "source": [
    "### Results Visualization\n",
    "Visualize the original image and the perceptual attacked image"
   ]
  },
  {
   "cell_type": "code",
   "id": "4b85c0f4",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rescale_array(array):\n",
    "    ele_min, ele_max = np.amin(array), np.amax(array)\n",
    "    array = (array - ele_min) / (ele_max - ele_min)\n",
    "    return array\n",
    "\n",
    "def tensor2img(tensor):\n",
    "    tensor = torch.nn.functional.interpolate(\n",
    "        tensor,\n",
    "        scale_factor=3,\n",
    "        mode=\"bilinear\"\n",
    "    )\n",
    "    array = tensor.detach().cpu().numpy()[0, :, :, :]\n",
    "    array = np.transpose(array, [1, 2, 0])\n",
    "    return array\n",
    "\n",
    "final_adv_input = torch.reshape(soln.final.x,inputs.shape)\n",
    "\n",
    "ori_image = rescale_array(tensor2img(inputs))\n",
    "adv_image = rescale_array(tensor2img(final_adv_input))\n",
    "\n",
    "f = plt.figure()\n",
    "f.add_subplot(1,2,1)\n",
    "plt.imshow(ori_image)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "f.add_subplot(1,2,2)\n",
    "plt.imshow(adv_image)\n",
    "plt.title('Adversarial Attacked Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
